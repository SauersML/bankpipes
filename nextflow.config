// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File
    models_csv                     = "${projectDir}/models.csv"

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    // GCS directories - WORKSPACE_BUCKET environment variable MUST be set where Nextflow runs.
    // The pipeline will fail if WORKSPACE_BUCKET is not set.
    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management
    pyspark_python_executable = "python3"
    spark_extra_conf_string = "--conf spark.sql.shuffle.partitions=400 --conf spark.sql.adaptive.enabled=true --conf spark.default.parallelism=32"

    // Hail & PySpark stack
    version_pyspark        = "3.3.2"
    version_hail           = "0.2.130.post1"
    version_nest_asyncio   = "1.5.8"

    // Core data science & GCS/BQ
    version_pandas         = "1.5.3"
    version_numpy          = "1.23.5"
    version_pyarrow        = "10.0.1"
    version_gcsfs          = "2023.10.0"
    version_google_cloud_bigquery = "3.11.4"
    version_db_dtypes      = "1.1.1"

    // Analysis & Visualization
    version_scikit_learn   = "1.2.2"
    version_scipy          = "1.10.1"
    version_matplotlib     = "3.7.0"
    version_seaborn        = "0.12.2"
    version_requests       = "2.31.0"
}

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    // WORKSPACE_BUCKET and WORKSPACE_CDR MUST be set in the environment where Nextflow is launched.
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    // Ensure pip --user installs are findable by adding ~/.local/bin to PATH.
    PATH             = "${System.getenv('HOME')}/.local/bin:${System.getenv('PATH')}"
    // Explicitly setting PYTHONPATH is avoided to prevent conflicts with Python's standard library discovery.
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker { enabled = false }
singularity { enabled = false }
pod { enabled = false }
process { container = null }


// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    executor = 'local'
    errorStrategy = 'retry'
    maxRetries    = 2

    // Common beforeScript for all Python tasks to ensure a consistent Python executable
    // and provide the check_install helper function.
    beforeScript = """
    #!/bin/bash
    set -e # Exit immediately if a command exits with a non-zero status.
    echo "INFO: Initializing Python environment for task: ${task.name}"
    PYTHON_EXE="${params.pyspark_python_executable}"

    # Verify PYTHON_EXE is Python 3
    if ! \${PYTHON_EXE} -V 2>&1 | grep -q "Python 3"; then
        echo "ERROR: PYTHON_EXE (set to '\${PYTHON_EXE}') does not appear to be Python 3. Please check params.pyspark_python_executable."
        exit 1
    fi

    # Function to check and install pip package to a specific version
    check_install() {
        local PACKAGE=\$1
        local VERSION_SPEC=\$2 # e.g., "==3.3.2", ">=1.5.8,<2", or exact version "1.5.8"
        local PKG_NAME_FOR_PIP=\$(echo "\$PACKAGE" | sed -E 's/[<>=!].*//') # Get base package name

        local INSTALLED_VERSION
        INSTALLED_VERSION=\$(\${PYTHON_EXE} -m pip show \${PKG_NAME_FOR_PIP} 2>/dev/null | grep '^Version:' | awk '{print \$2}')
        local NEEDS_INSTALL=true

        if [[ -n "\$INSTALLED_VERSION" ]]; then
            echo "INFO: Found \${PKG_NAME_FOR_PIP} version \${INSTALLED_VERSION}."
            # If VERSION_SPEC is an exact version (no operators)
            if [[ ! "\$VERSION_SPEC" =~ [\\<\\>\\=\\!~] ]]; then
                if [[ "\$INSTALLED_VERSION" == "\$VERSION_SPEC" ]]; then
                    echo "INFO: \${PKG_NAME_FOR_PIP} version \${INSTALLED_VERSION} matches required \${VERSION_SPEC}."
                    NEEDS_INSTALL=false
                else
                    echo "INFO: \${PKG_NAME_FOR_PIP} version \${INSTALLED_VERSION} does not match required exact version \${VERSION_SPEC}. Will reinstall."
                fi
            elif [[ "\$VERSION_SPEC" == ==* ]]; then # Exact version specified with ==
                local EXACT_VERSION_NEEDED=\${VERSION_SPEC#==}
                if [[ "\$INSTALLED_VERSION" == "\$EXACT_VERSION_NEEDED" ]]; then
                    echo "INFO: \${PKG_NAME_FOR_PIP} version \${INSTALLED_VERSION} matches required \${EXACT_VERSION_NEEDED}."
                    NEEDS_INSTALL=false
                else
                    echo "INFO: \${PACKAGE} version \${INSTALLED_VERSION} does not match required \${EXACT_VERSION_NEEDED}. Will upgrade/reinstall."
                fi
            else # For range specifiers or other operators, let pip handle resolution.
                 echo "INFO: \${PKG_NAME_FOR_PIP} is installed (version \${INSTALLED_VERSION}), but a version spec ('\${VERSION_SPEC}') is requested. Attempting install/upgrade to ensure compliance."
            fi
        else
            echo "INFO: \${PKG_NAME_FOR_PIP} not found."
        fi

        if [[ "\$NEEDS_INSTALL" == true ]]; then
            echo "INFO: Installing/Updating \${PACKAGE} (target spec: '${VERSION_SPEC}')..."
            # Construct the full package string for pip
            local PIP_INSTALL_STRING="\${PACKAGE}"
            if [[ ! "\$VERSION_SPEC" =~ [\\<\\>\\=\\!~] ]] && [[ -n "\$VERSION_SPEC" ]]; then # If version_spec is just a number, make it ==number
                PIP_INSTALL_STRING="\${PACKAGE}==\${VERSION_SPEC}"
            elif [[ -n "\$VERSION_SPEC" ]]; then
                PIP_INSTALL_STRING="\${PACKAGE}\${VERSION_SPEC}"
            fi

            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade "\${PIP_INSTALL_STRING}"; then
                 echo "INFO: \${PIP_INSTALL_STRING} install/update successful."
                 local NEW_INSTALLED_VERSION
                 NEW_INSTALLED_VERSION=\$(\${PYTHON_EXE} -m pip show \${PKG_NAME_FOR_PIP} 2>/dev/null | grep '^Version:' | awk '{print \$2}')
                 echo "INFO: Installed version of \${PKG_NAME_FOR_PIP} is now: \${NEW_INSTALLED_VERSION:-Not found after install attempt}"
            else
                 echo "ERROR: Failed to install/update \${PIP_INSTALL_STRING}. Exiting."
                 exit 1
            fi
        fi
    }
    """

    // LABEL: 'python_bq_env' - For tasks needing Pandas, GCS, BQ client (no Spark/Hail)
    withLabel: 'python_bq_env' {
        beforeScript += """
        echo "INFO: Setting up 'python_bq_env' packages..."
        check_install pandas "${params.version_pandas}"
        check_install numpy "${params.version_numpy}"
        check_install pyarrow "${params.version_pyarrow}"
        check_install requests "${params.version_requests}"
        check_install gcsfs "${params.version_gcsfs}"
        check_install google-cloud-bigquery "${params.version_google_cloud_bigquery}"
        check_install db-dtypes "${params.version_db_dtypes}"
        echo "INFO: 'python_bq_env' package setup complete."
        """
    }

    // LABEL: 'pyhail_env' - For processes needing Python/Hail and analysis dependencies
    withLabel: 'pyhail_env' {
        beforeScript += """
        echo "INFO: Setting up 'pyhail_env' (includes Spark/Hail and analysis tools) packages..."
        # -- Core Spark/Hail Dependencies --
        check_install pyspark "==${params.version_pyspark}" # Ensure exact match for pyspark
        check_install hail "==${params.version_hail}"     # Ensure exact match for hail
        check_install "nest-asyncio" "${params.version_nest_asyncio}"

        # -- Core Data Science & GCS/BQ --
        check_install pandas "${params.version_pandas}"
        check_install numpy "${params.version_numpy}"
        check_install pyarrow "${params.version_pyarrow}"
        check_install requests "${params.version_requests}"
        check_install gcsfs "${params.version_gcsfs}"
        check_install google-cloud-bigquery "${params.version_google_cloud_bigquery}"
        check_install db-dtypes "${params.version_db_dtypes}"

        # -- Analysis & Visualization Libraries --
        check_install scikit-learn "${params.version_scikit_learn}"
        check_install scipy "${params.version_scipy}"
        check_install matplotlib "${params.version_matplotlib}"
        check_install seaborn "${params.version_seaborn}"

        # PATH check for user-installed binaries, critical for finding pip-installed CLIs if any
        if [[ ":\$PATH:" != *":\${HOME}/.local/bin:"* ]]; then
            echo "WARN: \${HOME}/.local/bin not found in PATH. pip --user scripts might not be found by shell directly."
        fi

        echo "INFO: 'pyhail_env' package setup complete."
        echo "Python executable for this task: \$(\${PYTHON_EXE} -V)"
        echo "Selected installed packages (verify versions against params):"
        \${PYTHON_EXE} -m pip list | grep -E "pyspark|hail|pandas|numpy|scikit-learn|gcsfs|google-cloud-bigquery" || echo "Key packages check: Some packages might not be listed by grep if not installed."
        echo "----------------------------------------"
        """
    }

    // LABEL: 'spark_job' - For Spark/Hail processes.
    withLabel: 'spark_job' {
        env {
            PYSPARK_PYTHON = params.pyspark_python_executable
            // PYSPARK_SUBMIT_ARGS configured for cluster environments (like Dataproc).
            PYSPARK_SUBMIT_ARGS = "${params.spark_extra_conf_string}"
        }
    }


// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration' // Changed pcpu to %cpu
}
report {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
