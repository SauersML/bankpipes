// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File
    models_csv                     = "${projectDir}/models.csv"

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management
    pyspark_python_executable = "python" // Let Conda manage the specific python inside the env
    spark_extra_conf_string   = "--conf spark.sql.shuffle.partitions=400 --conf spark.sql.adaptive.enabled=true --conf spark.default.parallelism=32"

    // Hail & PySpark stack
    version_pyspark        = "3.3.2"
    version_hail           = "0.2.130.post1"
    version_nest_asyncio   = "1.5.8"

    // Core data science & GCS/BQ
    version_pandas             = "1.5.3"
    version_numpy              = "1.23.5"
    version_pyarrow            = "10.0.1"
    version_gcsfs              = "2023.10.0"
    version_google_cloud_bigquery = "3.11.4"
    version_db_dtypes          = "1.1.1"

    // Analysis & Visualization
    version_scikit_learn   = "1.2.2"
    version_scipy          = "1.10.1"
    version_matplotlib     = "3.7.0"
    version_seaborn        = "0.12.2"
    version_requests       = "2.31.0"

    // Conda settings
    conda_cache_dir        = "${System.getenv('HOME')}/.nextflow/conda_cache" // Or another persistent path
    use_mamba              = true // Recommended for speed
}

// NO Top-level Groovy variables defining report paths here

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    // PATH is typically managed by Conda environment activation
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker     { enabled = false }
singularity { enabled = false }
pod        { enabled = false }

// -------------------------------------------------------
// Executor Configuration - Use Local with Dynamic Max Parallelism
// -------------------------------------------------------
executor {
    name      = 'local'
    queueSize = Runtime.runtime.availableProcessors()
}

// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    executor      = 'local'
    errorStrategy = 'finish'
    maxRetries    = 1

    // Enable Conda globally for processes that request it via labels
    conda.enabled = true
    conda.useMamba = params.use_mamba // Use Mamba if specified
    conda.cacheDir = params.conda_cache_dir // Specify cache directory

    // Define Conda environments using labels
    withLabel: 'python_bq_env' {
        conda = """
                conda-forge::python=3.9 \
                conda-forge::pandas=${params.version_pandas} \
                conda-forge::numpy=${params.version_numpy} \
                conda-forge::pyarrow=${params.version_pyarrow} \
                conda-forge::requests=${params.version_requests} \
                conda-forge::gcsfs=${params.version_gcsfs} \
                pip::google-cloud-bigquery==${params.version_google_cloud_bigquery} \
                pip::db-dtypes==${params.version_db_dtypes}
                """
    }

    withLabel: 'pyhail_env' {
        conda = """
                conda-forge::python=3.9 \
                conda-forge::pandas=${params.version_pandas} \
                conda-forge::numpy=${params.version_numpy} \
                conda-forge::pyarrow=${params.version_pyarrow} \
                conda-forge::requests=${params.version_requests} \
                conda-forge::gcsfs=${params.version_gcsfs} \
                conda-forge::scikit-learn=${params.version_scikit_learn} \
                conda-forge::scipy=${params.version_scipy} \
                conda-forge::matplotlib=${params.version_matplotlib} \
                conda-forge::seaborn=${params.version_seaborn} \
                conda-forge::nest-asyncio=${params.version_nest_asyncio} \
                pip::pyspark==${params.version_pyspark} \
                pip::hail==${params.version_hail} \
                pip::google-cloud-bigquery==${params.version_google_cloud_bigquery} \
                pip::db-dtypes==${params.version_db_dtypes}
                """
    }

    withLabel: 'spark_job' {
        env {
            PYSPARK_SUBMIT_ARGS = "${params.spark_extra_conf_string}"
        }
    }
} // End of process scope

// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    // Use direct string interpolation WITHIN this scope
    file    = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    // Use direct string interpolation WITHIN this scope
    file    = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields  = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration'
}
report {
    enabled = true
    // Use direct string interpolation WITHIN this scope
    file    = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
