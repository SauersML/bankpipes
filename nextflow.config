// -------------------------------------------------------
// nextflow.config
// -------------------------------------------------------

// Enable Nextflow DSL2
nextflow {
    enable {
        dsl = 2
    }
    runName = "prs_analysis_env_driven_${new Date().format('yyyyMMdd')}_${workflow.sessionId.substring(0,8)}"
}

// -------------------------------------------------------
// Pipeline parameters
// These parameters directly use existing environment variables.
// The pipeline assumes these environment variables ARE SET in the execution environment.
// -------------------------------------------------------
params {
    // --- Core Configuration from MANDATORY Environment Variables ---
    google_billing_project    = System.getenv('GOOGLE_PROJECT')
    workspace_bucket          = System.getenv('WORKSPACE_BUCKET')
    aou_wgs_vds_path            = System.getenv('WGS_VDS_PATH') // Expected to be v8, e.g., gs://fc-aou-datasets-controlled/v8/...
    aou_workspace_cdr           = System.getenv('WORKSPACE_CDR')  // Expected to be current version, e.g., fc-aou-cdr-prod-ct.C2024Q3R5

    // Using the known v8 path directly.
    aou_flagged_samples_gcs_path= "gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv"

    // --- Analysis Specific User Parameters ---
    phenotype_concept_ids       = [432510, 374919] // for "Ischemic Stroke"
    target_phenotype_name       = 'Ischemic Stroke'
    models_csv                  = "${projectDir}/models.csv"

    // --- VDS Generation - Downsampling Settings ---
    enable_downsampling_for_vds_generation = true
    n_cases_downsample          = 500
    n_controls_downsample       = 500
    downsampling_random_state   = 2025

    // --- GCS Directories for Pipeline Operation (derived from workspace_bucket) ---
    // Base directory for Nextflow-managed temporary files (reusable VDS, PRS weights/intervals checkpoints)
    gcs_temp_dir_base           = "${params.workspace_bucket}/prs_analysis_temp_nf_env_local"
    // Base directory for Hail's specific temporary shuffle files (unique per run)
    gcs_hail_temp_dir_base      = "${params.workspace_bucket}/hail_temp_nf_env_local/${workflow.runName}"
    // Base directory for final run outputs (logs, scores, plots from Nextflow)
    output_dir_base             = "${params.workspace_bucket}/prs_analysis_runs_nf_env_local"

    // --- Python Execution ---
    // Uses PYSPARK_PYTHON from the environment.
    python_executable           = System.getenv('PYSPARK_PYTHON') // Path is /usr/local/bin/python3

    // --- Spark Configurations for Python Scripts ---
    // These configurations will be passed to Python scripts to be used in hl.init(conf={...}).
    spark_conf_list = [
        "spark.hadoop.fs.gs.requester.pays.mode=AUTO",
        "spark.hadoop.fs.gs.requester.pays.project.id=${params.google_billing_project}",
        "spark.hadoop.fs.gs.project.id=${params.google_billing_project}",
    ]
}

// --- Parameter Checks ---
// Verify that essential parameters derived from environment variables are actually set.
if (!params.google_billing_project)   { error "FATAL: GOOGLE_PROJECT environment variable is not set. Required for billing." }
if (!params.workspace_bucket)         { error "FATAL: WORKSPACE_BUCKET environment variable is not set. Required for GCS I/O." }
if (!params.aou_wgs_vds_path)         { error "FATAL: WGS_VDS_PATH environment variable is not set. Required for input VDS." }
if (!params.aou_workspace_cdr)        { error "FATAL: WORKSPACE_CDR environment variable is not set. Required for BigQuery." }
if (!params.python_executable)        { error "FATAL: PYSPARK_PYTHON environment variable is not set. Required for Python script execution." }

// -------------------------------------------------------
// No Containerization
// -------------------------------------------------------
docker.enabled                  = false
singularity.enabled             = false
podman.enabled                  = false
shifter.enabled                 = false
charliecloud.enabled            = false

// -------------------------------------------------------
// Executor configuration: Enforce local execution
// -------------------------------------------------------
executor {
    name                        = 'local'
    // Perhaps increase
    queueSize                   = Math.max(1, Runtime.runtime.availableProcessors().intdiv(2))
}

// -------------------------------------------------------
// Process defaults
// -------------------------------------------------------
process {
    executor                    = 'local' // Reinforce local execution for all processes
    errorStrategy               = 'finish'  // Default: finish processing other tasks if one fails
    maxRetries                  = 0         // No retries by default for local runs

    // The Python executable specified in params.python_executable (from PYSPARK_PYTHON env var) will be used.
}

// -------------------------------------------------------
// Reporting & Monitoring (output to run-specific directory under params.output_dir_base)
// -------------------------------------------------------
timeline {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields  = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration,attempt,tag'
}
report {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
dag {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/dag.html"
}
