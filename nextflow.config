// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File
    models_csv                     = "${projectDir}/models.csv"

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management
    pyspark_python_executable = "python3"
    spark_extra_conf_string = "--conf spark.sql.shuffle.partitions=400 --conf spark.sql.adaptive.enabled=true --conf spark.default.parallelism=32" // Kept as requested, relevance for local executor+Hail may vary.

    // Hail & PySpark stack
    version_pyspark        = "3.3.2"
    version_hail           = "0.2.130.post1"
    version_nest_asyncio   = "1.5.8" // Define lower bound for range

    // Core data science & GCS/BQ
    version_pandas         = "1.5.3"
    version_numpy          = "1.23.5"
    version_pyarrow        = "10.0.1"
    version_gcsfs          = "2023.10.0"
    version_google_cloud_bigquery = "3.11.4"
    version_db_dtypes      = "1.1.1"

    // Analysis & Visualization
    version_scikit_learn   = "1.2.2"
    version_scipy          = "1.10.1"
    version_matplotlib     = "3.7.0"
    version_seaborn        = "0.12.2"
    version_requests       = "2.31.0"
}

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    // WORKSPACE_BUCKET and WORKSPACE_CDR MUST be set in the environment where Nextflow is launched.
    // Runtime check added in beforeScript below.
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    PATH             = "${System.getenv('HOME')}/.local/bin:${System.getenv('PATH')}"
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker { enabled = false }
singularity { enabled = false }
pod { enabled = false }
// process { container = null } // This is implicit if docker/singularity/pod are disabled

// -------------------------------------------------------
// Executor Configuration - Use Local with Dynamic Max Parallelism
// -------------------------------------------------------
executor {
    name = 'local'
    // Allows Nextflow to SCHEDULE many tasks concurrently based on core count.
    queueSize = Runtime.runtime.availableProcessors()
    // pollInterval = '1 sec' // Can uncomment for potentially faster polling
    // queueStatInterval = '3 sec' // Can uncomment for more frequent queue status updates
}


// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    // Settings applying to ALL processes unless overridden by labels
    executor = 'local' // Explicitly confirm local executor
    errorStrategy = 'finish'
    maxRetries    = 1      // Limit retries

    // Remove ALL explicit Nextflow resource limits
    // cpus = null // no default is applied
    // memory = null // no default is applied

    // Common beforeScript for all Python tasks:
    // - Verifies Python environment.
    // - Checks for versions of PySpark/Hail, installs only if missing/wrong.
    // - Checks/installs other packages using pip's version resolution.
    // - Verifies essential ENV VARS are set.
    // - Installs packages via `pip install --user`.
    beforeScript = """
    #!/bin/bash
    set -e # Exit immediately if a command exits with a non-zero status.
    echo "INFO: Initializing Python environment for task: ${task.name} (PID: $$)"
    PYTHON_EXE="${params.pyspark_python_executable}"

    # Verify PYTHON_EXE is Python 3
    if ! \${PYTHON_EXE} -V 2>&1 | grep -q "Python 3"; then
        echo "ERROR: PYTHON_EXE (set to '\${PYTHON_EXE}') does not appear to be Python 3. Please check params.pyspark_python_executable."
        exit 1
    fi
    echo "INFO: Using Python: \$(\${PYTHON_EXE} --version)"
    echo "INFO: Python executable path: \$(command -v \${PYTHON_EXE} || echo 'Not in PATH')"
    echo "INFO: PIP version: \$(\${PYTHON_EXE} -m pip --version)"

    if [[ ":\$PATH:" != *":\${HOME}/.local/bin:"* ]]; then
        export PATH="\${HOME}/.local/bin:\${PATH}"
        echo "INFO: Added \${HOME}/.local/bin to PATH for task."
    fi

    # Function to check if EXACT version is installed, or install if missing/wrong
    check_install_exact() {
        local PACKAGE_NAME=\$1
        local EXACT_VERSION=\$2

        local INSTALLED_VERSION
        INSTALLED_VERSION=\$(${PYTHON_EXE} -m pip show \${PACKAGE_NAME} 2>/dev/null | grep '^Version:' | awk '{print \$2}')

        if [[ "\$INSTALLED_VERSION" == "\$EXACT_VERSION" ]]; then
            echo "INFO: Verified \${PACKAGE_NAME} version \${INSTALLED_VERSION} matches required \${EXACT_VERSION}."
        else
            if [[ -n "\$INSTALLED_VERSION" ]]; then
                 echo "WARN: Found \${PACKAGE_NAME} version \${INSTALLED_VERSION}, but require \${EXACT_VERSION}. Reinstalling..."
            else
                 echo "INFO: \${PACKAGE_NAME} not found. Installing required version \${EXACT_VERSION}..."
            fi
            # Use --no-deps strategically? Maybe not for spark/hail. Requires careful testing.
            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade --force-reinstall "\${PACKAGE_NAME}==\${EXACT_VERSION}"; then
                 echo "INFO: \${PACKAGE_NAME}==\${EXACT_VERSION} install/update successful."
                 local NEW_VERSION
                 NEW_VERSION=\$(${PYTHON_EXE} -m pip show \${PACKAGE_NAME} 2>/dev/null | grep '^Version:' | awk '{print \$2}')
                 if [[ "\$NEW_VERSION" == "\$EXACT_VERSION" ]]; then
                     echo "INFO: Confirmed installed version: \${NEW_VERSION}"
                 else
                     # This can happen if install fails subtly or environment is complex
                     echo "ERROR: Installation reported success, but could not confirm \${PACKAGE_NAME} version \${EXACT_VERSION} (\${NEW_VERSION} found). Check pip install logs carefully. Exiting."
                     exit 1
                 fi
            else
                 echo "ERROR: Failed to install \${PACKAGE_NAME}==\${EXACT_VERSION}. Exiting."
                 exit 1
            fi
        fi
    }

     # Function to check and install other packages (less strict version check, relies on pip resolution)
    check_install_other() {
        local PACKAGE_SPEC=\$1 # e.g., "pandas==1.5.3", "nest-asyncio>=1.5.8,<2"
        local PKG_NAME_FOR_CHECK=\$(echo "\$PACKAGE_SPEC" | sed -E 's/[<>=!].*//')

        # Simple check if package exists at all. Pip will handle version logic during install.
        # This avoids unnecessary checks/installs if ANY version exists, letting pip optimize later.
        if \${PYTHON_EXE} -m pip show \${PKG_NAME_FOR_CHECK} > /dev/null 2>&1; then
            # Package exists, let pip install handle the version spec update if needed
            echo "INFO: Package \${PKG_NAME_FOR_CHECK} found. Ensuring version spec '\${PACKAGE_SPEC}' is met..."
             if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade "\${PACKAGE_SPEC}"; then
                  : # Do nothing on success, pip handles versioning
             else
                  echo "ERROR: '\${PACKAGE_SPEC}'. Exiting."
                  exit 1
             fi
        else
            # Package not found at all, install it
            echo "INFO: Package \${PKG_NAME_FOR_CHECK} not found. Installing '\${PACKAGE_SPEC}'..."
            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade "\${PACKAGE_SPEC}"; then
                 echo "INFO: Install successful for '\${PACKAGE_SPEC}'"
            else
                 echo "ERROR: Failed to install package spec '\${PACKAGE_SPEC}'. Exiting."
                 exit 1
            fi
        fi
    }

    # --- Runtime Check for Essential Environment Variables ---
    # Fail EARLY if these are missing, as Python scripts depend on them.
    echo "INFO: Verifying required environment variables WORKSPACE_BUCKET and WORKSPACE_CDR..."
    if [[ -z "\${WORKSPACE_BUCKET:-}" ]]; then # Check if var is unset or empty
        echo "ERROR: Environment variable WORKSPACE_BUCKET is not set or is empty. It must be exported in the environment where Nextflow is launched."
        exit 1
    fi
     if [[ -z "\${WORKSPACE_CDR:-}" ]]; then # Check if var is unset or empty
        echo "ERROR: Environment variable WORKSPACE_CDR is not set or is empty. It must be exported in the environment where Nextflow is launched."
        exit 1
    fi
    echo "INFO: Confirmed WORKSPACE_BUCKET=\${WORKSPACE_BUCKET}"
    echo "INFO: Confirmed WORKSPACE_CDR=\${WORKSPACE_CDR}"
    echo "----------------------------------------" # End of common setup part

    """ // End of common beforeScript definition

    // LABEL: 'python_bq_env' - Appends specific package checks for BQ/GCS/Pandas tasks
    withLabel: 'python_bq_env' {
        beforeScript += """
        echo "INFO: Setting up 'python_bq_env' packages..."
        # Order matters less here, pip resolves dependencies
        check_install_other "pandas==${params.version_pandas}"
        check_install_other "numpy==${params.version_numpy}"
        check_install_other "pyarrow==${params.version_pyarrow}"
        check_install_other "requests==${params.version_requests}"
        check_install_other "gcsfs==${params.version_gcsfs}"
        check_install_other "google-cloud-bigquery==${params.version_google_cloud_bigquery}"
        check_install_other "db-dtypes==${params.version_db_dtypes}"
        echo "INFO: 'python_bq_env' package setup complete."
        echo "----------------------------------------"
        """
    }

    // LABEL: 'pyhail_env' - Appends specific checks for Hail/Spark stack + analysis tools
    withLabel: 'pyhail_env' {
        beforeScript += """
        echo "INFO: Setting up 'pyhail_env' (includes Spark/Hail and analysis tools) packages..."
        # -- Core Spark/Hail - Use EXACT versions (CRITICAL) --
        check_install_exact "pyspark" "${params.version_pyspark}"
        check_install_exact "hail" "${params.version_hail}"

        check_install_other "nest-asyncio>=${params.version_nest_asyncio},<2"
        check_install_other "pandas==${params.version_pandas}"
        check_install_other "numpy==${params.version_numpy}"
        check_install_other "pyarrow==${params.version_pyarrow}"
        check_install_other "requests==${params.version_requests}"
        check_install_other "gcsfs==${params.version_gcsfs}"
        check_install_other "google-cloud-bigquery==${params.version_google_cloud_bigquery}"
        check_install_other "db-dtypes==${params.version_db_dtypes}"
        check_install_other "scikit-learn==${params.version_scikit_learn}"
        check_install_other "scipy==${params.version_scipy}"
        check_install_other "matplotlib==${params.version_matplotlib}"
        check_install_other "seaborn==${params.version_seaborn}"

        echo "INFO: 'pyhail_env' package setup complete."
        echo "Python executable for this task: \$(\${PYTHON_EXE} -V)"
        echo "Selected installed packages check (versions):"
        \${PYTHON_EXE} -m pip list | grep -E 'pyspark|hail|nest-asyncio|pandas|numpy|gcsfs|google-cloud-bigquery|scikit-learn' || echo "Key packages check: Some packages might not be listed by grep."
        echo "----------------------------------------"
        """
    }

    // No Nextflow resource limits (cpus/memory) are set here.
    withLabel: 'spark_job' {
        env {
            PYSPARK_PYTHON = params.pyspark_python_executable
            PYSPARK_SUBMIT_ARGS = "${params.spark_extra_conf_string}"
        }
    }

} // End of process block


// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration' // Using %cpu
}
report {
    enabled = true
    file = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
