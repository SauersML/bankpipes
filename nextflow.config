// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]      // OMOP concept IDs for Ischemic Stroke
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File (Replaces models_list)
    models_csv                     = "${projectDir}/models.csv" // Ensure this path is correct

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    // GCS directories (all under your WORKSPACE_BUCKET)
    // Make sure WORKSPACE_BUCKET environment variable is set where Nextflow runs
    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management (optional, adjust based on your machine)
    max_spark_forks        = 1 // Max concurrent Hail/Spark processes on local executor
}

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    // preserve any existing PYTHONPATH and add user's local bin if needed for pip install --user
    PATH             = "${System.getenv('HOME')}/.local/bin:${System.getenv('PATH')}"
    PYTHONPATH       = "${System.getenv('HOME')}/.local/lib/python3.10/site-packages:${System.getenv('PYTHONPATH')}" // Adjust python version if needed
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker {
    enabled = false
}
singularity {
    enabled = false
}
pod {
    enabled = false
}
process {
    container = null
}


// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    // Default executor: local (runs on the machine where nextflow is launched)
    executor = 'local'

    // Default error strategy: retry on transient failures
    errorStrategy = 'retry'
    maxRetries    = 2

    // DO NOT specify global `cpus`, `memory`, or `time` here
    // to avoid constraining non-resource-intensive tasks unnecessarily.

    // LABEL: 'pyhail_env' - For processes needing Python/Hail dependencies
    // Installs packages using 'pip install --user'
    // Checks versions before installing to speed up reruns.
    withLabel: 'pyhail_env' {
        beforeScript = """
        #!/bin/bash
        set -e # Exit immediately if a command exits with a non-zero status.
        echo "INFO: [pgsline-nf] Checking Python environment..."
        PYTHON_EXE="python3" # Or specify python3.10 etc. if needed

        # Function to check pip package and version
        check_install() {
            PACKAGE=\$1
            VERSION_SPEC=\$2 # e.g., "==3.3.2" or ">=1.5.8,<2"
            VERSION_CHECK_CMD=""
            INSTALL_CMD="\${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade \\"\${PACKAGE}${VERSION_SPEC}\\""

            if [[ "\$VERSION_SPEC" == ==* ]]; then
                EXACT_VERSION=\${VERSION_SPEC#==}
                VERSION_CHECK_CMD="\${PYTHON_EXE} -m pip list | grep -E '^${PACKAGE}[[:space:]]+${EXACT_VERSION//./\\\\.}'"
            elif [[ "\$VERSION_SPEC" == >=* ]]; then
                 VERSION_CHECK_CMD="\${PYTHON_EXE} -m pip show \${PACKAGE} > /dev/null 2>&1" # Check if installed at all
                 echo "WARN: Cannot precisely check version spec '${VERSION_SPEC}' for ${PACKAGE}. Will install/upgrade if base package check fails."
            else
                 VERSION_CHECK_CMD="\${PYTHON_EXE} -m pip show \${PACKAGE} > /dev/null 2>&1" # Check if installed at all
                 echo "WARN: No version spec for ${PACKAGE}. Checking only for presence."
            fi


            echo "INFO: Checking for ${PACKAGE} (${VERSION_SPEC:-any version})..."
            if ${VERSION_CHECK_CMD}; then
                echo "INFO: ${PACKAGE} seems installed and matches spec."
            else
                echo "INFO: Installing/Updating ${PACKAGE} (${VERSION_SPEC:-latest}) ..."
                if ${INSTALL_CMD}; then
                     echo "INFO: ${PACKAGE} install/update successful."
                else
                     echo "ERROR: Failed to install/update ${PACKAGE}. Exiting."
                     exit 1
                fi
                # Verify after install
                 if ! ${VERSION_CHECK_CMD}; then
                    echo "WARN: ${PACKAGE} check still fails after install attempt. Check pip output and PATH/PYTHONPATH."
                    # Decide if this should be fatal depending on package criticality
                 fi
            fi
        }

        # -- Core Dependencies --
        check_install pyspark "==3.3.2"
        check_install hail "==0.2.130.post1" # Match Hail version to pyspark
        check_install "nest-asyncio" ">=1.5.8,<2"

        # -- Other Python Libraries --
        check_install pandas ""
        check_install numpy ""
        check_install requests ""
        check_install gcsfs ""
        check_install google-cloud-bigquery "" # For pandas-gbq
        check_install db-dtypes ""
        check_install scikit-learn ""
        check_install scipy ""
        check_install matplotlib ""
        check_install seaborn ""
        check_install pyarrow ""
        # This is also set in the 'env' block, but double-checking here can help debug path issues.
        if [[ ":\$PATH:" != *":\${HOME}/.local/bin:"* ]]; then
            echo "WARN: \${HOME}/.local/bin not found in PATH. pip --user scripts might not be found."
            # export PATH="\${HOME}/.local/bin:\${PATH}" # Optionally modify PATH for the task's script execution
        fi

        echo "INFO: Python environment check complete."
        echo "Python executable: \$(command -v \${PYTHON_EXE})"
        echo "pip executable: \$(command -v pip)"
        echo "Installed packages (first few lines):"
        \${PYTHON_EXE} -m pip list | head -n 10
        echo "----------------------------------------"
        """
    }

    // LABEL: 'spark_job'
    // We want max resource usage
    withLabel: 'spark_job' {
        maxForks = params.max_spark_forks
    }
}


// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file = "${workflow.launchDir}/results/reports/timeline.html"
}
trace {
    enabled = true
    file = "${workflow.launchDir}/results/reports/trace.txt"
    fields = 'task_id,name,status,exit,realtime,pcpu,rss,vmem,peak_rss,peak_vmem,workdir'
}
report {
    enabled = true
    file = "${workflow.launchDir}/results/reports/report.html"
}
