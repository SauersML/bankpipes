// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File
    models_csv                     = "${projectDir}/models.csv"

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management
    pyspark_python_executable = "python3"
    spark_extra_conf_string   = "--conf spark.sql.shuffle.partitions=400 --conf spark.sql.adaptive.enabled=true --conf spark.default.parallelism=32"

    // Hail & PySpark stack
    version_pyspark        = "3.3.2"
    version_hail           = "0.2.130.post1"
    version_nest_asyncio   = "1.5.8"

    // Core data science & GCS/BQ
    version_pandas             = "1.5.3"
    version_numpy              = "1.23.5"
    version_pyarrow            = "10.0.1"
    version_gcsfs              = "2023.10.0"
    version_google_cloud_bigquery = "3.11.4"
    version_db_dtypes          = "1.1.1"

    // Analysis & Visualization
    version_scikit_learn   = "1.2.2"
    version_scipy          = "1.10.1"
    version_matplotlib     = "3.7.0"
    version_seaborn        = "0.12.2"
    version_requests       = "2.31.0"
}

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    PATH             = "${System.getenv('HOME')}/.local/bin:${System.getenv('PATH')}"
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker     { enabled = false }
singularity { enabled = false }
pod        { enabled = false }

// -------------------------------------------------------
// Executor Configuration - Use Local with Dynamic Max Parallelism
// -------------------------------------------------------
executor {
    name      = 'local'
    queueSize = Runtime.runtime.availableProcessors()
}

// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    executor      = 'local'
    errorStrategy = 'finish'
    maxRetries    = 1

    beforeScript = """
    #!/bin/bash
    set -e

    echo "INFO: Initializing Python environment for task: ${task.name} (PID: \$\$)"
    PYTHON_EXE="${params.pyspark_python_executable}"

    if ! \${PYTHON_EXE} -V 2>&1 | grep -q "Python 3"; then
        echo "ERROR: PYTHON_EXE (set to '\${PYTHON_EXE}') does not appear to be Python 3. Please check params.pyspark_python_executable."
        exit 1
    fi
    echo "INFO: Using Python: \$(\${PYTHON_EXE} --version)"
    echo "INFO: Python executable path: \$(command -v \${PYTHON_EXE} || echo 'Not in PATH')"
    echo "INFO: PIP version: \$(\${PYTHON_EXE} -m pip --version)"

    if [[ ":\$PATH:" != *":\${HOME}/.local/bin:"* ]]; then
        export PATH="\${HOME}/.local/bin:\${PATH}"
        echo "INFO: Added \${HOME}/.local/bin to PATH for task."
    fi

    check_install_exact() {
        local PACKAGE_NAME=\$1
        local EXACT_VERSION=\$2
        local INSTALLED_VERSION
        INSTALLED_VERSION=\$(${PYTHON_EXE} -m pip show \${PACKAGE_NAME} 2>/dev/null | grep '^Version:' | awk '{print \$2}')

        if [[ "\$INSTALLED_VERSION" == "\$EXACT_VERSION" ]]; then
            echo "INFO: Verified \${PACKAGE_NAME} version \${INSTALLED_VERSION} matches required \${EXACT_VERSION}."
        else
            if [[ -n "\$INSTALLED_VERSION" ]]; then
                 echo "WARN: Found \${PACKAGE_NAME} version \${INSTALLED_VERSION}, but require \${EXACT_VERSION}. Reinstalling..."
            else
                 echo "INFO: \${PACKAGE_NAME} not found. Installing required version \${EXACT_VERSION}..."
            fi
            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade --force-reinstall "\${PACKAGE_NAME}==\${EXACT_VERSION}"; then
                 echo "INFO: \${PACKAGE_NAME}==\${EXACT_VERSION} install/update successful."
                 local NEW_VERSION
                 NEW_VERSION=\$(${PYTHON_EXE} -m pip show \${PACKAGE_NAME} 2>/dev/null | grep '^Version:' | awk '{print \$2}')
                 if [[ "\$NEW_VERSION" == "\$EXACT_VERSION" ]]; then
                     echo "INFO: Confirmed installed version: \${NEW_VERSION}"
                 else
                     echo "ERROR: Could not confirm \${PACKAGE_NAME} version \${EXACT_VERSION} (\${NEW_VERSION} found). Exiting."
                     exit 1
                 fi
            else
                 echo "ERROR: Failed to install \${PACKAGE_NAME}==\${EXACT_VERSION}. Exiting."
                 exit 1
            fi
        fi
    }

    check_install_other() {
        local PACKAGE_SPEC=\$1
        local PKG_NAME_FOR_CHECK=\$(echo "\$PACKAGE_SPEC" | sed -E 's/[<>=!].*//')

        if \${PYTHON_EXE} -m pip show \${PKG_NAME_FOR_CHECK} > /dev/null 2>&1; then
            echo "INFO: Package \${PKG_NAME_FOR_CHECK} found. Ensuring version spec '\${PACKAGE_SPEC}' is met..."
            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade "\${PACKAGE_SPEC}"; then
                :
            else
                echo "ERROR: '\${PACKAGE_SPEC}'. Exiting."
                exit 1
            fi
        else
            echo "INFO: Package \${PKG_NAME_FOR_CHECK} not found. Installing '\${PACKAGE_SPEC}'..."
            if \${PYTHON_EXE} -m pip install --user --no-cache-dir --upgrade "\${PACKAGE_SPEC}"; then
                 echo "INFO: Install successful for '\${PACKAGE_SPEC}'"
            else
                 echo "ERROR: Failed to install package spec '\${PACKAGE_SPEC}'. Exiting."
                 exit 1
            fi
        fi
    }

    echo "INFO: Verifying required environment variables WORKSPACE_BUCKET and WORKSPACE_CDR..."
    if [[ -z "\${WORKSPACE_BUCKET:-}" ]]; then
        echo "ERROR: Environment variable WORKSPACE_BUCKET is not set or is empty."
        exit 1
    fi
    if [[ -z "\${WORKSPACE_CDR:-}" ]]; then
        echo "ERROR: Environment variable WORKSPACE_CDR is not set or is empty."
        exit 1
    fi
    echo "INFO: Confirmed WORKSPACE_BUCKET=\${WORKSPACE_BUCKET}"
    echo "INFO: Confirmed WORKSPACE_CDR=\${WORKSPACE_CDR}"
    echo "----------------------------------------"
    """
    withLabel: 'python_bq_env' {
        beforeScript += """
        echo "INFO: Setting up 'python_bq_env' packages..."
        check_install_other "pandas==${params.version_pandas}"
        check_install_other "numpy==${params.version_numpy}"
        check_install_other "pyarrow==${params.version_pyarrow}"
        check_install_other "requests==${params.version_requests}"
        check_install_other "gcsfs==${params.version_gcsfs}"
        check_install_other "google-cloud-bigquery==${params.version_google_cloud_bigquery}"
        check_install_other "db-dtypes==${params.version_db_dtypes}"
        echo "INFO: 'python_bq_env' package setup complete."
        echo "----------------------------------------"
        """
    }
    withLabel: 'pyhail_env' {
        beforeScript += """
        echo "INFO: Setting up 'pyhail_env' (Spark/Hail + analysis tools) packages..."
        check_install_exact "pyspark" "${params.version_pyspark}"
        check_install_exact "hail" "${params.version_hail}"
        check_install_other "nest-asyncio>=${params.version_nest_asyncio},<2"
        check_install_other "pandas==${params.version_pandas}"
        check_install_other "numpy==${params.version_numpy}"
        check_install_other "pyarrow==${params.version_pyarrow}"
        check_install_other "requests==${params.version_requests}"
        check_install_other "gcsfs==${params.version_gcsfs}"
        check_install_other "google-cloud-bigquery==${params.version_google_cloud_bigquery}"
        check_install_other "db-dtypes==${params.version_db_dtypes}"
        check_install_other "scikit-learn==${params.version_scikit_learn}"
        check_install_other "scipy==${params.version_scipy}"
        check_install_other "matplotlib==${params.version_matplotlib}"
        check_install_other "seaborn==${params.version_seaborn}"
        echo "INFO: 'pyhail_env' package setup complete."
        echo "Python executable: \$(\${PYTHON_EXE} -V)"
        \${PYTHON_EXE} -m pip list | grep -E 'pyspark|hail|nest-asyncio|pandas|numpy|gcsfs|google-cloud-bigquery|scikit-learn' || echo "Key packages might not be listed."
        echo "----------------------------------------"
        """
    }
    withLabel: 'spark_job' {
        env {
            PYSPARK_PYTHON       = params.pyspark_python_executable
            PYSPARK_SUBMIT_ARGS  = "${params.spark_extra_conf_string}"
        }
    }
}

// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields  = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration'
}
report {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
