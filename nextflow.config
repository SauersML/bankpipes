// nextflow.config

// Enable Nextflow DSL2
nextflow.enable.dsl = 2

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // Input Data
    wgs_vds_path                   = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'
    flagged_samples_gcs_path       = 'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv'
    phenotype_concept_ids          = [432510, 374919]
    target_phenotype_name          = 'Ischemic Stroke'

    // Input Models File
    models_csv                     = "${projectDir}/models.csv"

    // Downsampling settings
    enable_downsampling_for_vds_generation = true
    n_cases_downsample                    = 500
    n_controls_downsample                 = 500
    downsampling_random_state             = 2025

    gcs_temp_dir_base      = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_temp_nextflow"
    gcs_hail_temp_dir_base = "${ System.getenv('WORKSPACE_BUCKET') }/hail_temp_nextflow"
    output_dir_base        = "${ System.getenv('WORKSPACE_BUCKET') }/prs_analysis_runs_nextflow"

    // Resource Management
    pyspark_python_executable = "python3"
    spark_extra_conf_string   = "--conf spark.sql.shuffle.partitions=400 --conf spark.sql.adaptive.enabled=true --conf spark.default.parallelism=32"

    // Hail & PySpark stack
    version_pyspark        = "3.3.2"
    version_hail           = "0.2.130.post1"
    version_nest_asyncio   = "1.5.8"

    // Core data science & GCS/BQ
    version_pandas             = "1.5.3"
    version_numpy              = "1.23.5"
    version_pyarrow            = "10.0.1"
    version_gcsfs              = "2023.10.0"
    version_google_cloud_bigquery = "3.11.4"
    version_db_dtypes          = "1.1.1"

    // Analysis & Visualization
    version_scikit_learn   = "1.2.2"
    version_scipy          = "1.10.1"
    version_matplotlib     = "3.7.0"
    version_seaborn        = "0.12.2"
    version_requests       = "2.31.0"
}

// -------------------------------------------------------
// Groovy Helper Definitions (MUST be outside config scopes like 'process')
// -------------------------------------------------------
// Define the path to the setup script
def setupScriptPath = "${projectDir}/scripts/setup_environment.sh"

// Define the base command arguments string using Groovy string concatenation (+)
// Accessing params here is okay at the top level.
def setupArgsBase = "'${params.pyspark_python_executable}'" +
    " '%TASK_LABEL%'" + // Placeholder for the label
    " '${params.version_pandas}'" +
    " '${params.version_numpy}'" +
    " '${params.version_pyarrow}'" +
    " '${params.version_requests}'" +
    " '${params.version_gcsfs}'" +
    " '${params.version_google_cloud_bigquery}'" +
    " '${params.version_db_dtypes}'" +
    " '${params.version_pyspark}'" +
    " '${params.version_hail}'" +
    " '${params.version_nest_asyncio}'" +
    " '${params.version_scikit_learn}'" +
    " '${params.version_scipy}'" +
    " '${params.version_matplotlib}'" +
    " '${params.version_seaborn}'"

// Function to generate the full beforeScript command including chmod
// This function is now defined globally within the config script context.
def generateBeforeScriptCommand(String label) {
    def specificArgs = setupArgsBase.replace('%TASK_LABEL%', label ?: "''")
    // Return the full command string
    return "chmod +x ${setupScriptPath} && bash ${setupScriptPath} ${specificArgs}"
}

// -------------------------------------------------------
// Environment variables exposed to tasks
// -------------------------------------------------------
env {
    WORKSPACE_BUCKET = System.getenv('WORKSPACE_BUCKET')
    WORKSPACE_CDR    = System.getenv('WORKSPACE_CDR')
    PATH             = "${System.getenv('HOME')}/.local/bin:${System.getenv('PATH')}"
}

// -------------------------------------------------------
// Disable any containerization: use bare metal
// -------------------------------------------------------
docker     { enabled = false }
singularity { enabled = false }
pod        { enabled = false }

// -------------------------------------------------------
// Executor Configuration - Use Local with Dynamic Max Parallelism
// -------------------------------------------------------
executor {
    name      = 'local'
    queueSize = Runtime.runtime.availableProcessors()
}

// -------------------------------------------------------
// Process defaults and specific configurations
// -------------------------------------------------------
process {
    // Standard process directives
    executor      = 'local'
    errorStrategy = 'finish'
    maxRetries    = 1

    // Use the globally defined helper function to set beforeScript
    beforeScript = generateBeforeScriptCommand('') // Default: empty label

    // Specific configurations for labels, overriding the default beforeScript
    withLabel: 'python_bq_env' {
        beforeScript = generateBeforeScriptCommand('python_bq_env')
    }

    withLabel: 'pyhail_env' {
        beforeScript = generateBeforeScriptCommand('pyhail_env')
    }

    // Specific environment settings for spark_job label
    // Note: 'env' is a valid directive within a process or withLabel scope.
    withLabel: 'spark_job' {
        env {
            PYSPARK_PYTHON       = params.pyspark_python_executable
            PYSPARK_SUBMIT_ARGS  = "${params.spark_extra_conf_string}"
        }
    }
} // End of process scope

// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields  = 'task_id,name,status,exit,realtime,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration'
}
report {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
