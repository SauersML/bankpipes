// -------------------------------------------------------
// nextflow.config
// -------------------------------------------------------

// Enable Nextflow DSL2
nextflow {
    enable {
        dsl = 2
    }
}

// -------------------------------------------------------
// Pipeline parameters
// -------------------------------------------------------
params {
    // --- Configuration from Environment Variables ---
    google_billing_project    = System.getenv('GOOGLE_PROJECT')
    workspace_bucket          = System.getenv('WORKSPACE_BUCKET')
    aou_wgs_vds_path            = System.getenv('WGS_VDS_PATH')
    aou_workspace_cdr           = System.getenv('WORKSPACE_CDR')
    aou_flagged_samples_gcs_path= "${System.getenv('CDR_STORAGE_PATH')}/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv"

    // --- User Parameters ---
    phenotype_concept_ids       = [432510, 374919]
    target_phenotype_name       = 'Ischemic Stroke'
    models_csv                  = "${projectDir}/models.csv"

    // --- VDS Generation - Downsampling Settings ---
    enable_downsampling_for_vds_generation = true
    n_cases_downsample          = 500
    n_controls_downsample       = 500
    downsampling_random_state   = 2025

    // --- GCS Directories for Pipeline Operation (derived from workspace_bucket) ---
    gcs_temp_dir_base           = "${params.workspace_bucket}/prs_analysis_temp_nf_env_local"
    gcs_hail_temp_dir_base      = "${params.workspace_bucket}/hail_temp_nf_env_local/${workflow.runName}"
    output_dir_base             = "${params.workspace_bucket}/prs_analysis_runs_nf_env_local"

    // --- Python Execution ---
    // Explicitly set to the Python working in the AoU environment.
    python_executable           = '/opt/conda/bin/python3'

    // --- Spark Configurations for Python Scripts (passed to hl.init(conf={...})) ---
    spark_conf_list = [
        "spark.hadoop.fs.gs.requester.pays.mode=AUTO",
        "spark.hadoop.fs.gs.requester.pays.project.id=${params.google_billing_project}",
        "spark.hadoop.fs.gs.project.id=${params.google_billing_project}"
    ]
}

// --- Mandatory Parameter Checks ---
if (!params.google_billing_project)   { error "FATAL: GOOGLE_PROJECT environment variable is not set. Required for billing." }
if (!params.workspace_bucket)         { error "FATAL: WORKSPACE_BUCKET environment variable is not set. Required for GCS I/O." }
if (!params.aou_wgs_vds_path)         { error "FATAL: WGS_VDS_PATH environment variable is not set. Required for input VDS." }
if (!System.getenv('CDR_STORAGE_PATH') && params.aou_flagged_samples_gcs_path.startsWith('${System.getenv(\'CDR_STORAGE_PATH\')}')) {
    error "FATAL: CDR_STORAGE_PATH environment variable is not set, but used for aou_flagged_samples_gcs_path construction."
}
if (!params.aou_workspace_cdr)        { error "FATAL: WORKSPACE_CDR environment variable is not set. Required for BigQuery." }
if (!(new File(params.python_executable).canExecute())) {
    error "FATAL: Configured python_executable ('${params.python_executable}') is not found or not executable. Check path."
}

// -------------------------------------------------------
// No Containerization or Nextflow-managed Conda
// -------------------------------------------------------
docker.enabled                  = false
singularity.enabled             = false
podman.enabled                  = false
shifter.enabled                 = false
charliecloud.enabled            = false

// -------------------------------------------------------
// Executor configuration
// -------------------------------------------------------
executor {
    name                        = 'local'
    queueSize                   = Math.max(1, Runtime.runtime.availableProcessors().intdiv(2))
}

// -------------------------------------------------------
// Process defaults
// -------------------------------------------------------
process {
    executor                    = 'local'
    errorStrategy               = 'finish'
    maxRetries                  = 0
    
    // Spark (and thus Hail) must use the correct Python interpreter.
    // This overrides any PYSPARK_PYTHON inherited from the parent environment if it's incorrect.
    beforeScript = """
    export PYSPARK_PYTHON="${params.python_executable}"
    echo "INFO: PYSPARK_PYTHON for Spark execution set to: ${params.python_executable}"
    """
}

// -------------------------------------------------------
// Reporting & Monitoring
// -------------------------------------------------------
timeline {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/timeline.html"
}
trace {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/trace.txt"
    fields  = 'task_id,name,status,exit,realtime,attempt,tag,%cpu,rss,vmem,peak_rss,peak_vmem,workdir,start,duration'
}
report {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/report.html"
}
dag {
    enabled = true
    file    = "${params.output_dir_base}/${workflow.runName}/reports/dag.html"
}
