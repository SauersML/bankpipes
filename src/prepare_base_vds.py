import argparse
import datetime
import os
import sys
import time
import pandas as pd
import hail as hl
from utils import (
    init_hail, gcs_path_exists, hail_path_exists, delete_gcs_path,
    cache_result, get_gcs_fs, get_cache_dir
)

def parse_args():
    """Parses command-line arguments for the VDS preparation script."""
    parser = argparse.ArgumentParser(description="Prepare Base Cohort VDS for PRS Analysis.")
    parser.add_argument("--google_billing_project", required=True, help="Google Cloud Project ID for billing and GCS access.")
    parser.add_argument("--workspace_cdr", required=True, help="Workspace CDR (e.g., fc-aou-cdr-prod-ct.CYYYYQQRR), used for querying WGS+EHR samples.")
    parser.add_argument("--run_timestamp", required=True, help="Run timestamp (YYYYMMDD_HHMMSS) for Hail logging.")
    parser.add_argument("--gcs_temp_dir", required=True, help="GCS base directory for stable intermediate checkpoints (VDS, ID lists).")
    parser.add_argument("--gcs_hail_temp_dir", required=True, help="GCS temporary directory specifically for Hail shuffle/intermediate operations.")
    parser.add_argument("--spark_configurations_json", required=True, help="JSON string of Spark configurations for Hail initialization.")
    parser.add_argument("--wgs_vds_path", required=True, help="GCS path to the full input WGS VDS.")
    parser.add_argument("--flagged_samples_gcs_path", required=True, help="GCS path to the TSV file containing flagged sample IDs.")
    parser.add_argument("--base_cohort_vds_path_out", required=True, help="GCS output path for the prepared base cohort VDS checkpoint.")
    parser.add_argument("--wgs_ehr_ids_gcs_path_out", required=True, help="GCS output path for the final list of sample IDs included in the base VDS (CSV format, 'person_id' column).")

    # Arguments related to downsampling logic
    parser.add_argument("--enable_downsampling_for_vds", action='store_true', help="Enable case/control downsampling during VDS generation.")
    parser.add_argument("--phenotype_cases_gcs_path_input", required=True, help="GCS path to the input CSV containing phenotype case definitions (requires 's' and 'phenotype_status' columns), generated by the fetch_phenotypes step. Used only if --enable_downsampling_for_vds is set.")
    parser.add_argument("--target_phenotype_name", required=True, help="Target phenotype name, primarily used for logging during downsampling.")
    parser.add_argument("--n_cases_downsample", type=int, default=500, help="Target number of cases for downsampling (if enabled).")
    parser.add_argument("--n_controls_downsample", type=int, default=500, help="Target number of controls for downsampling (if enabled).")
    parser.add_argument("--downsampling_random_state", type=int, default=2025, help="Random state seed for reproducible downsampling (if enabled).")
    parser.add_argument(
        "--hail_cluster_mode", 
        choices=["local", "dataproc_yarn"], 
        default="local", 
        help="Hail execution mode: 'local' for local Spark, 'dataproc_yarn' for running on a Dataproc YARN cluster."
    )
    return parser.parse_args()

# --- VDS Preparation Functions ---

def load_full_vds(path: str, fs_gcs) -> hl.vds.VariantDataset:
    """Loads the full WGS VDS from the specified path."""
    print(f"Loading full WGS VariantDataset from: {path}")
    try:
        # Hail's read_vds uses its own GCS connector configuration.
        vds = hl.vds.read_vds(path)
        
        # Perform a lightweight check to ensure the VDS is readable and has columns.
        # .cols().take(1) is much cheaper than .count_cols() on a large VDS.
        # It returns a list; an empty list means no columns (samples).
        if not vds.variant_data.cols().take(1):
            print(f"FATAL ERROR: Loaded VDS from {path} appears to have 0 samples (based on initial .cols().take(1) check).")
            sys.exit(1)

        print("VDS successfully loaded (initial column presence check passed).")
        # The full n_samples count will be expensive on the raw VDS. 
        print(f"Initial VDS variant_data n_partitions: {vds.variant_data.n_partitions()}")
        print(f"Initial VDS reference_data n_partitions: {vds.reference_data.n_partitions()}")
        try:
            # Log the entry schema of the variant_data component to understand its structure
            print(f"Initial VDS variant_data entry schema: {vds.variant_data.entry.dtype}\n")
        except Exception as schema_e:
            print(f"Could not retrieve VDS variant_data entry schema: {schema_e}\n")
        return vds
    except Exception as e:
        if "requester_pays" in str(e).lower():
            print(f"FATAL ERROR: Failed to load VDS from {path} due to requester pays issue. Check Hail/Spark GCS connector config. Error: {e}")
        else:
            print(f"FATAL ERROR: Failed to load or verify VDS from {path}: {e}")
        sys.exit(1)

def load_excluded_samples_ht(path: str, flagged_samples_gcs_path_default: str, fs_gcs) -> hl.Table | None:
    """Loads the table of flagged/excluded samples, returning None if not found or on error."""
    print(f"Importing flagged (related or excluded) samples from: {path}")
    if not gcs_path_exists(path): # Uses utils.gcs_path_exists
        error_message = f"ERROR: Flagged samples file not found or accessible at {path}. "
        if path == flagged_samples_gcs_path_default:
            error_message += "This is the default All of Us path. The file might have moved or permissions changed."
        else:
            error_message += "This is a custom path. Please check the path and permissions."
        print(error_message)
        print("CRITICAL INFO: Proceeding without relatedness/flagged sample filtering. This may impact analysis results.")
        return None
    try:
        # Impute types and set sample_id as key
        ht = hl.import_table(path, key='sample_id', impute=True)
        count = ht.count()
        print(f"Flagged samples loaded. Count: {count}\n")
        if count == 0:
            print("WARNING: Flagged samples table is empty.")
        return ht
    except Exception as e:
        print(f"ERROR: Failed to load flagged samples from {path}: {e}")
        print("Proceeding without relatedness filtering due to load error.")
        return None

def filter_samples_by_exclusion_list(vds: hl.vds.VariantDataset, excluded_ht: hl.Table | None) -> hl.vds.VariantDataset:
    """Filters samples present in the excluded_ht from the VDS."""
    if excluded_ht is None:
        print("Skipping relatedness/flagged sample filtering as excluded table is not available.")
        return vds # Return the original VDS if no exclusion list

    print("Filtering flagged samples out of the VDS...")
    try:
        n_before = vds.variant_data.count_cols()
        # the excluded table is keyed by 's' (string) to match VDS sample ID
        if 's' not in excluded_ht.key.dtype:
            print("Re-keying excluded_ht from 'sample_id' to 's' (string type).")
            # Assumes original key was 'sample_id'. Cast to string for safety.
            excluded_ht = excluded_ht.annotate(s=hl.str(excluded_ht.sample_id)).key_by('s')
        
        # Filter cols (samples) in both variant and reference data using anti-join logic
        cleaned_variant_data = vds.variant_data.filter_cols(
            hl.is_missing(excluded_ht[vds.variant_data.s])
        )
        cleaned_ref_data = vds.reference_data.filter_cols(
            hl.is_missing(excluded_ht[vds.reference_data.s])
        )
        
        # Create the new VDS with filtered data
        cleaned_vds = hl.vds.VariantDataset(cleaned_ref_data, cleaned_variant_data)
        n_after = cleaned_vds.variant_data.count_cols()
        print(f"Samples filtered. Count before: {n_before}, Count after: {n_after}. Removed: {n_before - n_after}\n")
        
        if n_after == 0:
            print("FATAL ERROR: Filtering removed all samples from the VDS.")
            sys.exit(1)
        return cleaned_vds
    except Exception as e:
        print(f"ERROR: Failed during sample filtering by exclusion list: {e}")
        print("Exiting due to failure during sample filtering.")
        sys.exit(1)

# Use local caching for the expensive BQ query to get WGS+EHR sample list
@cache_result("wgs_ehr_samples_df") # Decorator from utils for local pickle cache
def get_wgs_ehr_samples_df(cdr_env_var_value: str, prs_id=None) -> pd.DataFrame:
    """Fetches person_ids with both WGS and EHR data from BigQuery."""
    get_cache_dir() # local cache directory exists
    print("Retrieving WGS+EHR samples from BigQuery...")
    if not cdr_env_var_value:
        print(f"FATAL ERROR: Workspace CDR value not provided.")
        sys.exit(1)

    # This query identifies participants present in the WGS data release
    # who also have corresponding EHR data linked.
    wgs_ehr_query = f"""
    SELECT person_id
    FROM `{cdr_env_var_value}.person`
    WHERE person_id IN (
        SELECT DISTINCT person_id
        FROM `{cdr_env_var_value}.cb_search_person`
        WHERE has_ehr_data = 1
        AND has_whole_genome_variant = 1
    )
    """
    print("Executing BQ query for WGS+EHR samples.")
    try:
        # BigQuery client is configured (ADC, billing project)
        df = pd.read_gbq(wgs_ehr_query, dialect='standard', progress_bar_type=None)
        n_found = df['person_id'].nunique()
        print(f"WGS+EHR query completed. Found {n_found} unique persons.\n")
        if df.empty:
            print("FATAL ERROR: No samples found with both WGS and EHR data from BigQuery. Cannot proceed.")
            sys.exit(1)
        return df
    except Exception as e:
        print(f"FATAL ERROR: Failed to query BigQuery for WGS+EHR samples: {e}")
        sys.exit(1)

def save_sample_ids_to_gcs(df: pd.DataFrame, gcs_path: str, fs_gcs):
    """Saves the DataFrame of sample IDs (expected 'person_id' column) to GCS."""
    if df is None or df.empty:
        print("FATAL ERROR: Cannot save sample IDs DataFrame as it is None or empty.")
        sys.exit(1)
    if 'person_id' not in df.columns:
        print(f"FATAL ERROR: DataFrame to be saved to {gcs_path} must contain a 'person_id' column.")
        sys.exit(1)

    print(f"Storing sample IDs (column: 'person_id') to GCS: {gcs_path}")
    try:
        # parent directory exists
        parent_dir = os.path.dirname(gcs_path)
        if not gcs_path_exists(parent_dir):
             print(f"Creating GCS directory: {parent_dir}")
             fs_gcs.mkdirs(parent_dir, exist_ok=True)
             
        with fs_gcs.open(gcs_path, 'w') as f:
            # Save only the person_id column with header
            df[['person_id']].to_csv(f, index=False)
        print("Sample IDs saved successfully.\n")
    except Exception as e:
        print(f"FATAL ERROR: Failed to save sample IDs to {gcs_path}: {e}")
        sys.exit(1)

def filter_vds_to_sample_list(vds: hl.vds.VariantDataset, sample_ids_gcs_path: str, fs_gcs) -> hl.vds.VariantDataset:
    """Filters the VDS to include only samples present in the provided CSV list on GCS."""
    if vds is None:
        print("FATAL ERROR: VDS is None, cannot filter to sample list.")
        sys.exit(1)
    if not sample_ids_gcs_path:
        print("FATAL ERROR: Sample IDs GCS path is None or empty, cannot filter VDS.")
        sys.exit(1)

    print(f"Importing sample list from {sample_ids_gcs_path} for VDS filtering...")
    try:
        if not gcs_path_exists(sample_ids_gcs_path):
            print(f"FATAL ERROR: Sample IDs file not found at {sample_ids_gcs_path}")
            sys.exit(1)

        # Import the list into a HailTable, assuming 'person_id' header
        ids_ht = hl.import_table(sample_ids_gcs_path, delimiter=',', key='person_id', types={'person_id': hl.tstr})
        
        # Check if the table is empty after import
        if ids_ht.count() == 0:
            print(f"FATAL ERROR: Imported sample ID HailTable from {sample_ids_gcs_path} is empty. Cannot filter VDS.")
            sys.exit(1)

        # VDS sample key 's' is string type if not already
        # This modifies the VDS object in place if needed
        vds_variant_data = vds.variant_data
        vds_reference_data = vds.reference_data
        keys_cast = False
        if vds_variant_data.s.dtype != hl.tstr:
             print("Warning: VDS sample key 's' in variant_data is not string. Attempting cast.")
             vds_variant_data = vds_variant_data.key_cols_by(s=hl.str(vds_variant_data.s))
             keys_cast = True
        if vds_reference_data.s.dtype != hl.tstr:
             print("Warning: VDS sample key 's' in reference_data is not string. Attempting cast.")
             vds_reference_data = vds_reference_data.key_cols_by(s=hl.str(vds_reference_data.s))
             keys_cast = True
        if keys_cast:
             vds = hl.vds.VariantDataset(vds_reference_data, vds_variant_data)

        # Key the HT by 's' to match the VDS sample ID column name
        ids_ht_keyed = ids_ht.key_by(s=ids_ht.person_id)

        # Perform the filtering using semi_join on cols (keeps samples present in ids_ht_keyed)
        print("Filtering VDS variant_data to sample list...")
        filtered_variant_data = vds.variant_data.semi_join_cols(ids_ht_keyed)
        print("Filtering VDS reference_data to sample list...")
        filtered_reference_data = vds.reference_data.semi_join_cols(ids_ht_keyed)

        subset_vds = hl.vds.VariantDataset(filtered_reference_data, filtered_variant_data)

        # Verify outcome
        n_after_filter = subset_vds.variant_data.count_cols()
        print(f"VDS filtered to sample list. Final count: {n_after_filter}\n")
        if n_after_filter == 0:
             print(f"FATAL ERROR: Filtering VDS to sample list from {sample_ids_gcs_path} resulted in 0 samples remaining.")
             sys.exit(1)
        return subset_vds
    except Exception as e:
        print(f"FATAL ERROR: Failed filtering VDS to sample list: {e}")
        sys.exit(1)

def load_phenotype_cases_from_csv(gcs_path: str, fs_gcs) -> pd.DataFrame:
    """Loads the phenotype case definitions from the specified GCS CSV file."""
    print(f"Loading phenotype case data from CSV: {gcs_path}")
    if not gcs_path_exists(gcs_path):
        print(f"FATAL ERROR: Phenotype cases CSV file not found at {gcs_path}")
        sys.exit(1)
    try:
        with fs_gcs.open(gcs_path, 'r') as f:
            df = pd.read_csv(f)
        
        # Validate required columns
        required_cols = ['s', 'phenotype_status']
        if not all(col in df.columns for col in required_cols):
             print(f"FATAL ERROR: Phenotype cases CSV {gcs_path} must contain columns: {required_cols}. Found: {df.columns.tolist()}")
             sys.exit(1)
             
        # correct types
        df['s'] = df['s'].astype(str)
        df['phenotype_status'] = df['phenotype_status'].astype(int)
        
        print(f"Successfully loaded {len(df)} entries from phenotype cases CSV.")
        # Filter to only actual cases defined in the file (status == 1)
        cases_df = df[df['phenotype_status'] == 1].copy()
        print(f"Identified {len(cases_df)} cases from the CSV.")
        return cases_df
        
    except Exception as e:
        print(f"FATAL ERROR loading phenotype cases CSV from {gcs_path}: {e}")
        sys.exit(1)

# --- Main Script Execution ---

def main():
    args = parse_args()
    # local cache directory exists for functions using @cache_result
    get_cache_dir() 

    # Initialize GCS FileSystem and Hail
    # Explicitly pass billing project to GCSFileSystem initialization
    fs = get_gcs_fs(project_id_for_billing=args.google_billing_project)
    init_hail(
        gcs_hail_temp_dir=args.gcs_hail_temp_dir,
        log_suffix=args.run_timestamp, # Using run_timestamp as the log suffix for this script
        spark_configurations_json_str=args.spark_configurations_json,
        cluster_mode=args.hail_cluster_mode # Pass the cluster mode to Hail initialization
    )
    # Set a default number of partitions for Hail operations to improve GCS I/O and prevent too many small files.
    # This value scales with the available Spark cores to remain efficient on both small and large clusters.
    dynamic_partitions = max(200, hl.spark_context().defaultParallelism * 4)
    # Set Hail's default partition count in a way that is compatible across versions.
    try:
        hl.default_n_partitions(dynamic_partitions)
    except AttributeError:
        if hasattr(hl, "set_default_n_partitions"):
            hl.set_default_n_partitions(dynamic_partitions)

    base_cohort_vds = None # Initialize VDS variable

    # --- Check for Existing VDS Checkpoint ---
    # Uses the specified output path as the checkpoint location
    if hail_path_exists(args.base_cohort_vds_path_out): # Uses utils.hail_path_exists
        print(f"[CHECKPOINT] Found potential Base Cohort VDS directory at: {args.base_cohort_vds_path_out}")
        try:
            print("Attempting to read VDS checkpoint...")
            base_cohort_vds = hl.vds.read_vds(args.base_cohort_vds_path_out)
            # Perform a quick verification count
            n_samples = base_cohort_vds.variant_data.count_cols()
            n_variants = base_cohort_vds.variant_data.count_rows()
            print(f"Sanity check on loaded VDS: Found {n_samples} samples and {n_variants} variants.")
            if n_samples > 0: # Only need samples for this VDS to be useful
                print("[CHECKPOINT HIT] Successfully loaded and verified VDS checkpoint.\n")
            else:
                print("[CHECKPOINT CORRUPTED] Loaded VDS has 0 samples. Assuming invalid.")
                base_cohort_vds = None # Force regeneration
                # Attempt to delete the corrupted checkpoint directory
                if not delete_gcs_path(args.base_cohort_vds_path_out, recursive=True): # Uses utils.delete_gcs_path
                     print(f"WARNING: Failed to delete corrupted VDS checkpoint at {args.base_cohort_vds_path_out}")
        except Exception as e:
            error_message = str(e)
            print(f"[CHECKPOINT CORRUPTED] Failed to read VDS from {args.base_cohort_vds_path_out}. Error: {error_message}")
            # Be specific about corruption errors vs other issues
            if "corrupted" in error_message.lower() or "metadata.json.gz is missing" in error_message or "Not a VDS" in error_message:
                print("Error suggests checkpoint corruption. Attempting deletion and regeneration...")
                base_cohort_vds = None # Force regeneration
                if not delete_gcs_path(args.base_cohort_vds_path_out, recursive=True):
                     print(f"WARNING: Failed to delete corrupted VDS checkpoint at {args.base_cohort_vds_path_out}")
            else:
                # For other errors (permissions, network), exit rather than deleting potentially good data
                print("Error reading VDS checkpoint was not identified as typical corruption. Exiting to investigate.")
                sys.exit(1)

    # --- Regenerate VDS if Checkpoint Loading Failed or VDS is None ---
    if base_cohort_vds is None:
        print(f"[CHECKPOINT MISS or CORRUPTED] Base Cohort VDS needs to be generated at {args.base_cohort_vds_path_out}.")
        print("--- Starting Base VDS Generation ---")

        # 1. Load Full VDS
        full_vds = load_full_vds(args.wgs_vds_path, fs)

        # 2. Load Excluded Samples List
        excluded_ht = load_excluded_samples_ht(args.flagged_samples_gcs_path, args.flagged_samples_gcs_path, fs) # Pass default path for comparison message

        # 3. Filter Excluded Samples from VDS
        cleaned_vds = filter_samples_by_exclusion_list(full_vds, excluded_ht)
        del full_vds, excluded_ht # Free memory

        # 4. Get the list of all individuals with WGS and EHR data (uses local cache)
        all_wgs_ehr_individuals_df = get_wgs_ehr_samples_df(args.workspace_cdr)
        # Prepare this DataFrame for potential merging: rename 'person_id' to 's', string type
        all_wgs_ehr_individuals_df = all_wgs_ehr_individuals_df.rename(columns={'person_id': 's'})
        all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str)

        # 5. Determine the final list of sample IDs to keep in the VDS
        final_ids_for_vds_df = None # DataFrame with 's' column
        
        if args.enable_downsampling_for_vds:
            print(f"--- Downsampling enabled for VDS generation ---")
            print(f"Target: ~{args.n_cases_downsample} cases / ~{args.n_controls_downsample} controls for phenotype '{args.target_phenotype_name}'")
            
            # Load phenotype case definitions from the upstream step's output CSV
            phenotype_cases_df = load_phenotype_cases_from_csv(args.phenotype_cases_gcs_path_input, fs)
            # phenotype_cases_df contains 's' and 'phenotype_status' (where status is 1)

            # Define cases and controls *within the WGS+EHR cohort*
            # Merge the full WGS+EHR list with the loaded cases
            merged_cohort_phenotype_df = pd.merge(
                all_wgs_ehr_individuals_df[['s']], # Start with all WGS+EHR individuals
                phenotype_cases_df[['s', 'phenotype_status']], # Bring in case status
                on='s',
                how='left' # Keep all WGS+EHR individuals
            )
            # Fill NA status (meaning not a case) with 0 (control)
            merged_cohort_phenotype_df['phenotype_status'] = merged_cohort_phenotype_df['phenotype_status'].fillna(0).astype(int)
            
            print(f"Total WGS+EHR individuals available for downsampling: {len(merged_cohort_phenotype_df)}")
            print(f"Phenotype distribution within WGS+EHR cohort:\n{merged_cohort_phenotype_df['phenotype_status'].value_counts(dropna=False)}")

            # Separate cases and controls from the merged cohort
            cases_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 1]
            controls_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 0]
            print(f"Available cases in WGS+EHR cohort: {len(cases_in_cohort_df)}")
            print(f"Available controls in WGS+EHR cohort: {len(controls_in_cohort_df)}")

            # Perform sampling
            num_cases_to_sample = min(args.n_cases_downsample, len(cases_in_cohort_df))
            sampled_cases_df = cases_in_cohort_df.sample(n=num_cases_to_sample, random_state=args.downsampling_random_state, replace=False)
            print(f"Sampled {len(sampled_cases_df)} cases.")

            num_controls_to_sample = min(args.n_controls_downsample, len(controls_in_cohort_df))
            sampled_controls_df = controls_in_cohort_df.sample(n=num_controls_to_sample, random_state=args.downsampling_random_state, replace=False)
            print(f"Sampled {len(sampled_controls_df)} controls.")

            # Combine and get the final list of sample IDs ('s' column)
            final_ids_for_vds_df = pd.concat([sampled_cases_df, sampled_controls_df], ignore_index=True)[['s']]
            final_ids_for_vds_df = final_ids_for_vds_df.drop_duplicates(subset=['s'])
            print(f"Total unique individuals in downsampled cohort for VDS generation: {len(final_ids_for_vds_df)}")
            
            if final_ids_for_vds_df.empty:
                print("FATAL ERROR: Downsampling resulted in an empty cohort. Check N_cases/N_controls and available data.")
                sys.exit(1)
            
            # Clean up intermediate dataframes
            del phenotype_cases_df, merged_cohort_phenotype_df, cases_in_cohort_df, controls_in_cohort_df, sampled_cases_df, sampled_controls_df

        else:
            # If downsampling is not enabled, use all WGS+EHR individuals
            print("--- Using full WGS+EHR cohort for VDS generation (downsampling disabled) ---")
            final_ids_for_vds_df = all_wgs_ehr_individuals_df[['s']] # Use the 's' column

        # 6. Prepare the final ID list for saving (needs 'person_id' column) and filter the VDS
        # Rename 's' back to 'person_id' for saving the list
        df_to_save_for_vds_filter = final_ids_for_vds_df.rename(columns={'s': 'person_id'})
        
        # Save this definitive list of IDs that will be in the VDS
        save_sample_ids_to_gcs(df_to_save_for_vds_filter, args.wgs_ehr_ids_gcs_path_out, fs) 
        
        # Filter the VDS using this list (passed via GCS path)
        base_cohort_vds = filter_vds_to_sample_list(cleaned_vds, args.wgs_ehr_ids_gcs_path_out, fs) 

        # Clean up intermediate dataframes
        del all_wgs_ehr_individuals_df, final_ids_for_vds_df, df_to_save_for_vds_filter, cleaned_vds

        # 7. Write the Filtered VDS as a Checkpoint
        print(f"Writing filtered Base Cohort VDS checkpoint to: {args.base_cohort_vds_path_out}")
        try:
            # Repartition the VDS to a more manageable number of partitions before writing.
            # This improves GCS I/O performance for the checkpoint and subsequent reads.
            # Retrieve the previously configured default partition count in a version-agnostic way.
            try:
                target_vds_partitions = hl.default_n_partitions()
            except AttributeError:
                if hasattr(hl, "set_default_n_partitions"):
                    hl.set_default_n_partitions(dynamic_partitions)
                target_vds_partitions = dynamic_partitions
            
            current_variant_partitions = base_cohort_vds.variant_data.n_partitions()
            current_reference_partitions = base_cohort_vds.reference_data.n_partitions()

            print(f"Current VDS partitions before checkpoint write: variant_data={current_variant_partitions}, reference_data={current_reference_partitions}")
            
            # Only repartition if significantly different, to avoid unnecessary shuffles
            # Heuristic
            if current_variant_partitions > target_vds_partitions:
                print(f"Repartitioning VDS to ~{target_vds_partitions} variant_data partitions before writing checkpoint...")
                # For reference_data, fewer partitions are often sufficient as it's usually smaller or accessed differently.
                # A fraction of target_vds_partitions, though at least 1.
                target_ref_partitions = max(1, target_vds_partitions // 10) 
                
                # Check if reference data actually needs repartitioning
                ref_data_repartitioned = base_cohort_vds.reference_data
                if current_reference_partitions > target_ref_partitions:
                     print(f"Repartitioning reference_data to {target_ref_partitions} partitions.")
                     ref_data_repartitioned = base_cohort_vds.reference_data.repartition(target_ref_partitions, shuffle=True)

                print(f"Repartitioning variant_data to {target_vds_partitions} partitions.")
                variant_data_repartitioned = base_cohort_vds.variant_data.repartition(target_vds_partitions, shuffle=True)
                
                base_cohort_vds = hl.vds.VariantDataset(ref_data_repartitioned, variant_data_repartitioned)
                print(f"VDS repartitioned. New partitions: variant_data={base_cohort_vds.variant_data.n_partitions()}, reference_data={base_cohort_vds.reference_data.n_partitions()}")

            # Overwrite if attempting regeneration due to previous failure
            base_cohort_vds.write(args.base_cohort_vds_path_out, overwrite=True) 
            print("Base Cohort VDS checkpoint successfully written.")
            
            # Verify the written checkpoint minimally
            print("Verifying written checkpoint...")
            vds_check = hl.vds.read_vds(args.base_cohort_vds_path_out)
            vds_check_sample_count = vds_check.variant_data.count_cols()
            print(f"Checkpoint verified successfully. Sample count: {vds_check_sample_count}")
            del vds_check # Clean up verification object
        except Exception as e:
            print(f"ERROR: Failed to write or verify Base Cohort VDS checkpoint: {e}")
            print("FATAL ERROR: Checkpoint write/verification failed. Deleting potentially incomplete checkpoint.")
            # Attempt cleanup of the failed write
            delete_gcs_path(args.base_cohort_vds_path_out, recursive=True) 
            sys.exit(1)
            
        print("--- Base VDS Generation Finished ---")

    # --- Final Check and Exit ---
    # base_cohort_vds is valid before exiting script successfully
    if base_cohort_vds is None:
        # This should only happen if checkpoint existed but failed verification AND regeneration also failed.
        print(f"FATAL ERROR: base_cohort_vds is None after attempting load/regeneration from {args.base_cohort_vds_path_out}. Cannot proceed.")
        sys.exit(1)
    else:
        # Double-check sample count just before exiting
        final_sample_count = base_cohort_vds.variant_data.count_cols()
        if final_sample_count == 0:
            print(f"FATAL ERROR: Final base_cohort_vds at {args.base_cohort_vds_path_out} has 0 samples.")
            sys.exit(1)
        print(f"Base Cohort VDS at {args.base_cohort_vds_path_out} is ready with {final_sample_count} samples.\n")

    # If execution reaches here, the VDS exists (either loaded or generated) and the sample ID list is saved.
    print(f"Script completed successfully.")
    print(f"Output Base VDS path: {args.base_cohort_vds_path_out}")
    print(f"Output Sample ID list path: {args.wgs_ehr_ids_gcs_path_out}")

if __name__ == "__main__":
    main()
