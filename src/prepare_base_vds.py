import argparse
import datetime
import os
import sys
import time
import pandas as pd
import hail as hl
from utils import (
    init_hail, gcs_path_exists, hail_path_exists, delete_gcs_path,
    cache_result, get_gcs_fs, get_cache_dir
)

def parse_args():
    """Parses command-line arguments for the VDS preparation script."""
    parser = argparse.ArgumentParser(description="Prepare Base Cohort VDS for PRS Analysis.")
    parser.add_argument("--google_billing_project", required=True, help="Google Cloud Project ID for billing and GCS access.")
    parser.add_argument("--workspace_cdr", required=True, help="Workspace CDR (e.g., fc-aou-cdr-prod-ct.CYYYYQQRR), used for querying WGS+EHR samples.")
    parser.add_argument("--run_timestamp", required=True, help="Run timestamp (YYYYMMDD_HHMMSS) for Hail logging.")
    parser.add_argument("--gcs_temp_dir", required=True, help="GCS base directory for stable intermediate checkpoints (VDS, ID lists).")
    parser.add_argument("--gcs_hail_temp_dir", required=True, help="GCS temporary directory specifically for Hail shuffle/intermediate operations.")
    parser.add_argument("--wgs_vds_path", required=True, help="GCS path to the full input WGS VDS.")
    parser.add_argument("--flagged_samples_gcs_path", required=True, help="GCS path to the TSV file containing flagged sample IDs.")
    parser.add_argument("--base_cohort_vds_path_out", required=True, help="GCS output path for the prepared base cohort VDS checkpoint.")
    parser.add_argument("--wgs_ehr_ids_gcs_path_out", required=True, help="GCS output path for the final list of sample IDs included in the base VDS (CSV format, 'person_id' column).")

    # Arguments related to downsampling logic
    parser.add_argument("--enable_downsampling_for_vds", action='store_true', help="Enable case/control downsampling during VDS generation.")
    parser.add_argument("--phenotype_cases_gcs_path_input", required=True, help="GCS path to the input CSV containing phenotype case definitions (requires 's' and 'phenotype_status' columns), generated by the fetch_phenotypes step. Used only if --enable_downsampling_for_vds is set.")
    parser.add_argument("--target_phenotype_name", required=True, help="Target phenotype name, primarily used for logging during downsampling.")
    parser.add_argument("--n_cases_downsample", type=int, default=500, help="Target number of cases for downsampling (if enabled).")
    parser.add_argument("--n_controls_downsample", type=int, default=500, help="Target number of controls for downsampling (if enabled).")
    parser.add_argument("--downsampling_random_state", type=int, default=2025, help="Random state seed for reproducible downsampling (if enabled).")
    return parser.parse_args()

# --- VDS Preparation Functions ---

def load_full_vds(path: str, fs_gcs) -> hl.vds.VariantDataset:
    """Loads the full WGS VDS from the specified path."""
    print(f"Loading full WGS VariantDataset from: {path}")
    try:
        # Hail's read_vds uses its own GCS connector configuration.
        vds = hl.vds.read_vds(path)
        
        if not vds.variant_data.cols().take(1):
            print(f"FATAL ERROR: Loaded VDS from {path} appears to have 0 samples (based on initial .cols().take(1) check).")
            sys.exit(1)

        print("VDS successfully loaded (initial column presence check passed).")
        print(f"Initial VDS variant_data n_partitions: {vds.variant_data.n_partitions()}")
        print(f"Initial VDS reference_data n_partitions: {vds.reference_data.n_partitions()}")
        try:
            print(f"Initial VDS variant_data entry schema: {vds.variant_data.entry.dtype}\n")
        except Exception as schema_e:
            print(f"Could not retrieve VDS variant_data entry schema: {schema_e}\n")
        return vds
    except Exception as e:
        if "requester_pays" in str(e).lower():
            print(f"FATAL ERROR: Failed to load VDS from {path} due to requester pays issue. Check Hail/Spark GCS connector config. Error: {e}")
        else:
            print(f"FATAL ERROR: Failed to load or verify VDS from {path}: {e}")
        sys.exit(1)

def load_excluded_samples_ht(path: str, flagged_samples_gcs_path_default: str, fs_gcs) -> hl.Table | None:
    """Loads the table of flagged/excluded samples, returning None if not found or on error."""
    print(f"Importing flagged (related or excluded) samples from: {path}")
    if not gcs_path_exists(path): 
        error_message = f"ERROR: Flagged samples file not found or accessible at {path}. "
        if path == flagged_samples_gcs_path_default:
            error_message += "This is the default All of Us path. The file might have moved or permissions changed."
        else:
            error_message += "This is a custom path. Please check the path and permissions."
        print(error_message)
        print("CRITICAL INFO: Proceeding without relatedness/flagged sample filtering. This may impact analysis results.")
        return None
    try:
        ht = hl.import_table(path, key='sample_id', impute=True)
        count = ht.count()
        print(f"Flagged samples loaded. Count: {count}\n")
        if count == 0:
            print("WARNING: Flagged samples table is empty.")
        return ht
    except Exception as e:
        print(f"ERROR: Failed to load flagged samples from {path}: {e}")
        print("Proceeding without relatedness filtering due to load error.")
        return None

def filter_samples_by_exclusion_list(
    vds: hl.vds.VariantDataset, 
    excluded_ht: hl.Table | None, 
    id_column_name: str = 'sample_id'
) -> hl.vds.VariantDataset:
    print(f"[FILTER_EXCLUSION] Starting sample filtering. Initial VDS variant_data sample count: {vds.variant_data.count_cols()}.") # Action
    
    if excluded_ht is None:
        print("[FILTER_EXCLUSION] Exclusion table is None. Returning original VDS.")
        return vds

    initial_excluded_count = excluded_ht.count() # Action
    print(f"[FILTER_EXCLUSION] Exclusion table received with {initial_excluded_count} entries.")
    if initial_excluded_count == 0:
        print("[FILTER_EXCLUSION] Exclusion table is empty. Returning original VDS.")
        return vds

    if id_column_name not in excluded_ht.row:
        msg = f"[FILTER_EXCLUSION] ERROR: ID column '{id_column_name}' not found in exclusion table. Columns: {list(excluded_ht.row.keys())}"
        print(msg)
        raise ValueError(msg)

    print(f"[FILTER_EXCLUSION] Preparing exclusion table. Original key: {list(excluded_ht.key.keys()) if excluded_ht.key is not None else 'None'}, target ID field for rekey: '{id_column_name}'.")
    
    temp_s_col = '_s_to_key_' # temp column name for string conversion
    
    if id_column_name == 's' and excluded_ht[id_column_name].dtype == hl.tstr:
        excluded_ht_rekeyed = excluded_ht.key_by('s')
        print(f"[FILTER_EXCLUSION] Exclusion table already has 's' key of type string.")
    elif id_column_name != 's' and excluded_ht[id_column_name].dtype == hl.tstr:
        excluded_ht_rekeyed = excluded_ht.key_by(id_column_name).rename({id_column_name: 's'})
        print(f"[FILTER_EXCLUSION] Exclusion table keyed by '{id_column_name}' (string) and key column renamed to 's'.")
    else: 
        print(f"[FILTER_EXCLUSION] Exclusion table field '{id_column_name}' (type {excluded_ht[id_column_name].dtype}) needs conversion/rekeying.")
        excluded_ht_rekeyed = excluded_ht.annotate(**{temp_s_col: hl.str(excluded_ht[id_column_name])})
        excluded_ht_rekeyed = excluded_ht_rekeyed.key_by(temp_s_col).rename({temp_s_col: 's'})
        print(f"[FILTER_EXCLUSION] Exclusion table converted to string, keyed by and key column renamed to 's'.")

    print(f"[FILTER_EXCLUSION] Schema of re-keyed exclusion table: {excluded_ht_rekeyed.row.dtype}, Key: {list(excluded_ht_rekeyed.key.keys())}")
    
    print(f"[FILTER_EXCLUSION] Persisting re-keyed exclusion table...")
    try:
        excluded_ht_rekeyed = excluded_ht_rekeyed.persist()
        count_after_persist = excluded_ht_rekeyed.count() # Action
        print(f"[FILTER_EXCLUSION] Re-keyed exclusion table persisted. Count: {count_after_persist}.")
        if initial_excluded_count != count_after_persist:
            print(f"[FILTER_EXCLUSION] WARNING: Count mismatch after persist. Before: {initial_excluded_count}, After: {count_after_persist}")
    except Exception as e:
        print(f"[FILTER_EXCLUSION] ERROR during persist: {e}. Continuing without persisted table.")

    vd_initial_cols = vds.variant_data.count_cols() # Action
    print(f"[FILTER_EXCLUSION] Filtering variant_data (samples: {vd_initial_cols}, partitions: {vds.variant_data.n_partitions()})...")
    try:
        if vds.variant_data.s.dtype != hl.tstr:
            print(f"[FILTER_EXCLUSION] WARNING: VDS variant_data 's' column is not string ({vds.variant_data.s.dtype}). This might cause issues if exclusion keys are strings.")
        filtered_variant_data = vds.variant_data.filter_cols(hl.is_missing(excluded_ht_rekeyed[vds.variant_data.s]))
        vd_cols_after_filter = filtered_variant_data.count_cols() # Action
        print(f"[FILTER_EXCLUSION] Variant_data filtered. Samples remaining: {vd_cols_after_filter}. Partitions: {filtered_variant_data.n_partitions()}.")
    except Exception as e:
        print(f"[FILTER_EXCLUSION] ERROR filtering variant_data: {e}")
        raise

    rd_initial_cols = vds.reference_data.count_cols() # Action
    print(f"[FILTER_EXCLUSION] Filtering reference_data (samples: {rd_initial_cols}, partitions: {vds.reference_data.n_partitions()})...")
    try:
        if vds.reference_data.s.dtype != hl.tstr:
             print(f"[FILTER_EXCLUSION] WARNING: VDS reference_data 's' column is not string ({vds.reference_data.s.dtype}).")
        filtered_reference_data = vds.reference_data.filter_cols(hl.is_missing(excluded_ht_rekeyed[vds.reference_data.s]))
        rd_cols_after_filter = filtered_reference_data.count_cols() # Action
        print(f"[FILTER_EXCLUSION] Reference_data filtered. Samples remaining: {rd_cols_after_filter}. Partitions: {filtered_reference_data.n_partitions()}.")
    except Exception as e:
        print(f"[FILTER_EXCLUSION] ERROR filtering reference_data: {e}")
        raise

    if vd_cols_after_filter != rd_cols_after_filter:
        print(f"[FILTER_EXCLUSION] WARNING: Sample count mismatch after filtering! Variant data: {vd_cols_after_filter}, Reference data: {rd_cols_after_filter}")

    print("[FILTER_EXCLUSION] Creating new VariantDataset...")
    new_vds = hl.vds.VariantDataset(filtered_reference_data, filtered_variant_data)
    print(f"[FILTER_EXCLUSION] New VariantDataset created. Final sample count (from variant_data): {vd_cols_after_filter}.")
    return new_vds

@cache_result("wgs_ehr_samples_df") 
def get_wgs_ehr_samples_df(cdr_env_var_value: str, prs_id=None) -> pd.DataFrame:
    get_cache_dir() 
    print("Retrieving WGS+EHR samples from BigQuery...")
    if not cdr_env_var_value:
        print(f"FATAL ERROR: Workspace CDR value not provided.")
        sys.exit(1)

    wgs_ehr_query = f"""
    SELECT person_id
    FROM `{cdr_env_var_value}.person`
    WHERE person_id IN (
        SELECT DISTINCT person_id
        FROM `{cdr_env_var_value}.cb_search_person`
        WHERE has_ehr_data = 1
        AND has_whole_genome_variant = 1
    )
    """
    print("Executing BQ query for WGS+EHR samples.")
    try:
        df = pd.read_gbq(wgs_ehr_query, dialect='standard', progress_bar_type=None)
        n_found = df['person_id'].nunique()
        print(f"WGS+EHR query completed. Found {n_found} unique persons.\n")
        if df.empty:
            print("FATAL ERROR: No samples found with both WGS and EHR data from BigQuery. Cannot proceed.")
            sys.exit(1)
        return df
    except Exception as e:
        print(f"FATAL ERROR: Failed to query BigQuery for WGS+EHR samples: {e}")
        sys.exit(1)

def save_sample_ids_to_gcs(df: pd.DataFrame, gcs_path: str, fs_gcs):
    if df is None or df.empty:
        print("FATAL ERROR: Cannot save sample IDs DataFrame as it is None or empty.")
        sys.exit(1)
    if 'person_id' not in df.columns:
        print(f"FATAL ERROR: DataFrame to be saved to {gcs_path} must contain a 'person_id' column.")
        sys.exit(1)

    print(f"Storing sample IDs (column: 'person_id') to GCS: {gcs_path}")
    try:
        parent_dir = os.path.dirname(gcs_path)
        if not gcs_path_exists(parent_dir): # fs_gcs is not passed here, gcs_path_exists will use global or re-init
             print(f"Creating GCS directory: {parent_dir}")
             # fs_gcs should be used if available and appropriate
             fs_gcs.mkdirs(parent_dir, exist_ok=True) # fs_gcs passed to parent function
             
        with fs_gcs.open(gcs_path, 'w') as f: # fs_gcs passed to parent function
            df[['person_id']].to_csv(f, index=False)
        print("Sample IDs saved successfully.\n")
    except Exception as e:
        print(f"FATAL ERROR: Failed to save sample IDs to {gcs_path}: {e}")
        sys.exit(1)

def filter_vds_to_sample_list(vds: hl.vds.VariantDataset, sample_ids_gcs_path: str, fs_gcs) -> hl.vds.VariantDataset:
    if vds is None:
        print("FATAL ERROR: VDS is None, cannot filter to sample list.")
        sys.exit(1)
    if not sample_ids_gcs_path:
        print("FATAL ERROR: Sample IDs GCS path is None or empty, cannot filter VDS.")
        sys.exit(1)

    print(f"Importing sample list from {sample_ids_gcs_path} for VDS filtering...")
    try:
        # fs_gcs not used here, gcs_path_exists will use its own logic
        if not gcs_path_exists(sample_ids_gcs_path):
            print(f"FATAL ERROR: Sample IDs file not found at {sample_ids_gcs_path}")
            sys.exit(1)

        ids_ht = hl.import_table(sample_ids_gcs_path, delimiter=',', key='person_id', types={'person_id': hl.tstr})
        
        if ids_ht.count() == 0: # Action
            print(f"FATAL ERROR: Imported sample ID HailTable from {sample_ids_gcs_path} is empty. Cannot filter VDS.")
            sys.exit(1)

        vds_variant_data = vds.variant_data
        vds_reference_data = vds.reference_data
        keys_cast = False
        if vds_variant_data.s.dtype != hl.tstr:
             print("Warning: VDS sample key 's' in variant_data is not string. Attempting cast.")
             vds_variant_data = vds_variant_data.key_cols_by(s=hl.str(vds_variant_data.s))
             keys_cast = True
        if vds_reference_data.s.dtype != hl.tstr:
             print("Warning: VDS sample key 's' in reference_data is not string. Attempting cast.")
             vds_reference_data = vds_reference_data.key_cols_by(s=hl.str(vds_reference_data.s))
             keys_cast = True
        if keys_cast:
             vds = hl.vds.VariantDataset(vds_reference_data, vds_variant_data)

        ids_ht_keyed = ids_ht.key_by(s=ids_ht.person_id)

        print("Filtering VDS variant_data to sample list...")
        filtered_variant_data = vds.variant_data.semi_join_cols(ids_ht_keyed)
        print("Filtering VDS reference_data to sample list...")
        filtered_reference_data = vds.reference_data.semi_join_cols(ids_ht_keyed)

        subset_vds = hl.vds.VariantDataset(filtered_reference_data, filtered_variant_data)

        n_after_filter = subset_vds.variant_data.count_cols() # Action
        print(f"VDS filtered to sample list. Final count: {n_after_filter}\n")
        if n_after_filter == 0:
             print(f"FATAL ERROR: Filtering VDS to sample list from {sample_ids_gcs_path} resulted in 0 samples remaining.")
             sys.exit(1)
        return subset_vds
    except Exception as e:
        print(f"FATAL ERROR: Failed filtering VDS to sample list: {e}")
        sys.exit(1)

def load_phenotype_cases_from_csv(gcs_path: str, fs_gcs) -> pd.DataFrame:
    print(f"Loading phenotype case data from CSV: {gcs_path}")
    # fs_gcs not used here for exists check
    if not gcs_path_exists(gcs_path):
        print(f"FATAL ERROR: Phenotype cases CSV file not found at {gcs_path}")
        sys.exit(1)
    try:
        with fs_gcs.open(gcs_path, 'r') as f: # fs_gcs used here
            df = pd.read_csv(f)
        
        required_cols = ['s', 'phenotype_status']
        if not all(col in df.columns for col in required_cols):
             print(f"FATAL ERROR: Phenotype cases CSV {gcs_path} must contain columns: {required_cols}. Found: {df.columns.tolist()}")
             sys.exit(1)
             
        df['s'] = df['s'].astype(str)
        df['phenotype_status'] = df['phenotype_status'].astype(int)
        
        print(f"Successfully loaded {len(df)} entries from phenotype cases CSV.")
        cases_df = df[df['phenotype_status'] == 1].copy()
        print(f"Identified {len(cases_df)} cases from the CSV.")
        return cases_df
        
    except Exception as e:
        print(f"FATAL ERROR loading phenotype cases CSV from {gcs_path}: {e}")
        sys.exit(1)

# --- Main Script Execution ---

def main():
    args = parse_args()
    get_cache_dir() 

    fs = get_gcs_fs(project_id_for_billing=args.google_billing_project)
    init_hail(
        gcs_hail_temp_dir=args.gcs_hail_temp_dir,
        log_suffix=args.run_timestamp 
    )
    dynamic_partitions = max(200, hl.spark_context().defaultParallelism * 4)
    try:
        hl.default_n_partitions(dynamic_partitions)
    except AttributeError:
        if hasattr(hl, "set_default_n_partitions"):
            hl.set_default_n_partitions(dynamic_partitions)

    base_cohort_vds = None 
    # Define the path for the old intermediate checkpoint format, which is now obsolete
    OLD_TRUNCATED_VDS_CHECKPOINT_PATH = args.base_cohort_vds_path_out.replace(".vds", "_truncated_checkpoint.vds")

    if hail_path_exists(args.base_cohort_vds_path_out, project_id_for_billing=args.google_billing_project): 
        print(f"[CHECKPOINT] Found potential Base Cohort VDS directory at: {args.base_cohort_vds_path_out}")
        try:
            print("Attempting to read VDS checkpoint...")
            base_cohort_vds = hl.vds.read_vds(args.base_cohort_vds_path_out)
            n_samples = base_cohort_vds.variant_data.count_cols() # Action
            n_variants = base_cohort_vds.variant_data.count_rows() # Action
            print(f"Sanity check on loaded VDS: Found {n_samples} samples and {n_variants} variants.")
            if n_samples > 0 and n_variants > 0: # Ensure VDS is not empty
                print("[CHECKPOINT HIT] Successfully loaded and verified VDS checkpoint.\n")
                # If we hit a valid checkpoint, clean up the OLD intermediate path if it exists, as it's no longer used by current logic
                if hail_path_exists(OLD_TRUNCATED_VDS_CHECKPOINT_PATH, project_id_for_billing=args.google_billing_project):
                    print(f"INFO: Valid main checkpoint loaded. Cleaning up old/obsolete intermediate path: {OLD_TRUNCATED_VDS_CHECKPOINT_PATH}")
                    if not delete_gcs_path(OLD_TRUNCATED_VDS_CHECKPOINT_PATH, project_id_for_billing=args.google_billing_project, recursive=True):
                        print(f"WARNING: Failed to delete old intermediate VDS checkpoint at {OLD_TRUNCATED_VDS_CHECKPOINT_PATH}")
            else:
                print(f"[CHECKPOINT CORRUPTED/EMPTY] Loaded VDS has 0 samples or 0 variants. Samples: {n_samples}, Variants: {n_variants}. Assuming invalid.")
                base_cohort_vds = None # Force regeneration
                print(f"Attempting to delete corrupted/empty VDS checkpoint at {args.base_cohort_vds_path_out}")
                if not delete_gcs_path(args.base_cohort_vds_path_out, project_id_for_billing=args.google_billing_project, recursive=True):
                     print(f"WARNING: Failed to delete corrupted/empty VDS checkpoint at {args.base_cohort_vds_path_out}")
        except Exception as e:
            error_message = str(e)
            print(f"[CHECKPOINT LOAD FAILED] Failed to read VDS from {args.base_cohort_vds_path_out}. Error: {error_message}")
            base_cohort_vds = None # Force regeneration
            print(f"Attempting to delete VDS checkpoint at {args.base_cohort_vds_path_out} due to read error.")
            if not delete_gcs_path(args.base_cohort_vds_path_out, project_id_for_billing=args.google_billing_project, recursive=True):
                 print(f"WARNING: Failed to delete VDS checkpoint at {args.base_cohort_vds_path_out} after read error.")

    # If, after all checks, base_cohort_vds is still None, it means we need to generate it.
    if base_cohort_vds is None:
        print(f"[DECISION] Base Cohort VDS needs to be generated/regenerated at {args.base_cohort_vds_path_out}.")

        # --- AGGRESSIVE AUTO-DELETION OF OLD/BAD STUFF BEFORE REGENERATION ---
        print("INFO: Preparing for VDS regeneration. Ensuring target paths are clear...")
        paths_to_clean_before_regen = [args.base_cohort_vds_path_out, OLD_TRUNCATED_VDS_CHECKPOINT_PATH]
        for path_to_clean in paths_to_clean_before_regen:
            if hail_path_exists(path_to_clean, project_id_for_billing=args.google_billing_project): # Check existence before attempting delete
                print(f"INFO: Attempting to delete existing path: '{path_to_clean}'.")
                if not delete_gcs_path(path_to_clean, project_id_for_billing=args.google_billing_project, recursive=True):
                    print(f"WARNING: Failed to delete '{path_to_clean}'. If this is the main output, 'overwrite=True' will attempt to handle. If intermediate, it might be orphaned.")
            else:
                print(f"INFO: Path '{path_to_clean}' does not exist. No cleanup needed for this path.")
        # --- END AUTO-DELETION ---

        print("--- Starting Base VDS Generation ---")

        full_vds = load_full_vds(args.wgs_vds_path, fs)
        excluded_ht = load_excluded_samples_ht(args.flagged_samples_gcs_path, args.flagged_samples_gcs_path, fs)
        
        # This section for filtering by exclusion list and then by target cohort
        # needs to result in 'cleaned_vds' and 'final_ids_for_vds_df' for the next steps.
        # The user's provided code implies these variables are formed correctly before the VDS construction.
        print("Starting process to filter VDS by exclusion list...")
        initial_vds_sample_count = -1
        try:
            initial_vds_sample_count = full_vds.variant_data.count_cols() # Action
            print(f"Full VDS sample count before exclusion filter: {initial_vds_sample_count}")
        except Exception as e:
            print(f"ERROR: Could not get initial VDS sample count: {e}")
        
        cleaned_vds = filter_samples_by_exclusion_list(
            full_vds, 
            excluded_ht, 
            id_column_name='sample_id' 
        )
        del full_vds, excluded_ht

        # Logic to define final_ids_for_vds_df (either from WGS+EHR or downsampling)
        all_wgs_ehr_individuals_df = get_wgs_ehr_samples_df(args.workspace_cdr)
        all_wgs_ehr_individuals_df = all_wgs_ehr_individuals_df.rename(columns={'person_id': 's'})
        all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str)

        final_ids_for_vds_df = None # Initialize
        # This is inside the 'if base_cohort_vds is None:' block
        # 'all_wgs_ehr_individuals_df' should be defined before this.
        # 'final_ids_for_vds_df' is initialized to None before this conditional block.

        if args.enable_downsampling_for_vds:
            print(f"--- Downsampling enabled for VDS generation ---")
            print(f"Target: ~{args.n_cases_downsample} cases / ~{args.n_controls_downsample} controls for phenotype '{args.target_phenotype_name}'")
            
            phenotype_cases_df = load_phenotype_cases_from_csv(args.phenotype_cases_gcs_path_input, fs)

            # Ensure 's' column is string type for merging
            if 's' not in all_wgs_ehr_individuals_df.columns:
                print("FATAL ERROR: 's' column missing from all_wgs_ehr_individuals_df before downsampling merge.")
                sys.exit(1)
            all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str)
            
            if 's' not in phenotype_cases_df.columns:
                print("FATAL ERROR: 's' column missing from phenotype_cases_df before downsampling merge.")
                sys.exit(1)
            phenotype_cases_df['s'] = phenotype_cases_df['s'].astype(str)

            merged_cohort_phenotype_df = pd.merge(
                all_wgs_ehr_individuals_df[['s']], 
                phenotype_cases_df[['s', 'phenotype_status']], 
                on='s',
                how='left' 
            )
            # Fill NA for phenotype_status with 0 (controls) and ensure it's integer
            merged_cohort_phenotype_df['phenotype_status'] = merged_cohort_phenotype_df['phenotype_status'].fillna(0).astype(int)
            
            print(f"Total WGS+EHR individuals available for downsampling: {len(merged_cohort_phenotype_df)}")
            print(f"Phenotype distribution within WGS+EHR cohort (1=case, 0=control/unknown):\n{merged_cohort_phenotype_df['phenotype_status'].value_counts(dropna=False)}")

            cases_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 1]
            controls_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 0]
            print(f"Available cases in WGS+EHR cohort: {len(cases_in_cohort_df)}")
            print(f"Available controls in WGS+EHR cohort: {len(controls_in_cohort_df)}")

            num_cases_to_sample = min(args.n_cases_downsample, len(cases_in_cohort_df))
            if len(cases_in_cohort_df) > 0 and num_cases_to_sample > 0 :
                 sampled_cases_df = cases_in_cohort_df.sample(n=num_cases_to_sample, random_state=args.downsampling_random_state, replace=False)
            else:
                 sampled_cases_df = pd.DataFrame(columns=['s', 'phenotype_status']) # Empty df
            print(f"Sampled {len(sampled_cases_df)} cases.")

            num_controls_to_sample = min(args.n_controls_downsample, len(controls_in_cohort_df))
            if len(controls_in_cohort_df) > 0 and num_controls_to_sample > 0:
                sampled_controls_df = controls_in_cohort_df.sample(n=num_controls_to_sample, random_state=args.downsampling_random_state, replace=False)
            else:
                sampled_controls_df = pd.DataFrame(columns=['s', 'phenotype_status']) # Empty df
            print(f"Sampled {len(sampled_controls_df)} controls.")

            final_ids_for_vds_df = pd.concat([sampled_cases_df, sampled_controls_df], ignore_index=True)[['s']]
            final_ids_for_vds_df = final_ids_for_vds_df.drop_duplicates(subset=['s'])
            print(f"Total unique individuals in downsampled cohort for VDS generation: {len(final_ids_for_vds_df)}")
            
            if final_ids_for_vds_df.empty and (num_cases_to_sample > 0 or num_controls_to_sample > 0) : # Only error if we intended to sample but got none
                print("FATAL ERROR: Downsampling resulted in an empty cohort, but cases or controls were expected. Check N_cases/N_controls and available data.")
                sys.exit(1)
            elif final_ids_for_vds_df.empty:
                 print("INFO: Downsampling resulted in an empty cohort (no cases/controls requested or available).")


            # Clean up DataFrames
            del phenotype_cases_df, merged_cohort_phenotype_df, cases_in_cohort_df, controls_in_cohort_df, sampled_cases_df, sampled_controls_df
        else:
            # This is the 'else' for 'if args.enable_downsampling_for_vds:'
            # This part should already exist in the code from the previous full main() replacement
            print("--- Using full WGS+EHR cohort for VDS generation (downsampling disabled) ---")
            final_ids_for_vds_df = all_wgs_ehr_individuals_df[['s']] 

        # Filter 'cleaned_vds' to 'final_ids_for_vds_df'
        print(f"Converting {len(final_ids_for_vds_df)} target sample IDs to HailTable for VDS filtering...")
        target_samples_ht = hl.Table.from_pandas(final_ids_for_vds_df).key_by('s') # Assumes final_ids_for_vds_df has 's' column

        print(f"Filtering VDS (after flagged/related removal) to the {target_samples_ht.count()} target samples...") # Action
        vd_target_cohort_filtered = cleaned_vds.variant_data.filter_cols(
            hl.is_defined(target_samples_ht[cleaned_vds.variant_data.s])
        )
        rd_target_cohort_filtered = cleaned_vds.reference_data.filter_cols(
            hl.is_defined(target_samples_ht[cleaned_vds.reference_data.s])
        )
        current_base_vds = hl.vds.VariantDataset(rd_target_cohort_filtered, vd_target_cohort_filtered)
        
        current_base_vds = current_base_vds.persist('MEMORY_AND_DISK')
        print("INFO: Persisted current_base_vds to memory and disk.")

        del cleaned_vds, target_samples_ht, vd_target_cohort_filtered, rd_target_cohort_filtered

        num_samples_in_vds = current_base_vds.variant_data.count_cols() # Action
        print(f"VDS filtered to target cohort. Samples: {num_samples_in_vds}.")
        print(f"Initial partitions BEFORE any optimization: variant_data rows: {current_base_vds.variant_data.n_partitions()}, reference_data rows: {current_base_vds.reference_data.n_partitions()}")

        if num_samples_in_vds == 0:
            print("FATAL ERROR: Filtering VDS to target sample list resulted in 0 samples remaining.")
            sys.exit(1)
        if hasattr(final_ids_for_vds_df, '__len__') and num_samples_in_vds != len(final_ids_for_vds_df):
             print(f"WARNING: Sample count mismatch! Expected {len(final_ids_for_vds_df)} from ID list, got {num_samples_in_vds} in VDS.")

        # -------- START INSANELY MASSIVE SPEEDUP SECTION (Applied to current_base_vds) --------
        vd_current_n_partitions = current_base_vds.variant_data.n_partitions()
        target_vd_partitions = dynamic_partitions 

        optimized_variant_data = current_base_vds.variant_data
        if vd_current_n_partitions > target_vd_partitions * 1.5:
            print(f"INFO: Repartitioning variant_data from {vd_current_n_partitions} to {target_vd_partitions} partitions.")
            optimized_variant_data = optimized_variant_data.repartition(target_vd_partitions, shuffle=True)
            print(f"INFO: variant_data repartitioned. New partitions: {optimized_variant_data.n_partitions()}")
        else:
            print(f"INFO: variant_data already has {vd_current_n_partitions} partitions (target ~{target_vd_partitions}). No repartitioning needed.")

        MAX_REF_BLOCK_LENGTH = 10000
        print(f"Truncating reference blocks to max length {MAX_REF_BLOCK_LENGTH} bp...")
        temp_vds_for_truncation = hl.vds.VariantDataset(current_base_vds.reference_data, optimized_variant_data)
        
        vds_final_optimized = hl.vds.truncate_reference_blocks(
            temp_vds_for_truncation, 
            max_ref_block_base_pairs=MAX_REF_BLOCK_LENGTH
        )
        del current_base_vds, temp_vds_for_truncation, optimized_variant_data

        num_variants_final_vd = vds_final_optimized.variant_data.count_rows() # Action
        print(f"Final VDS optimized. Samples: {num_samples_in_vds}, Variants (in variant_data): {num_variants_final_vd}.")
        print(f"Final partitions: variant_data rows: {vds_final_optimized.variant_data.n_partitions()}, reference_data rows: {vds_final_optimized.reference_data.n_partitions()}")

        print(f"Writing final prepared and OPTIMIZED Base Cohort VDS to: {args.base_cohort_vds_path_out}")
        try:
            vds_final_optimized.write(args.base_cohort_vds_path_out, overwrite=True) 
            print("Base Cohort VDS (optimized) checkpoint successfully written.")

            print("Verifying written (optimized) checkpoint...")
            vds_check = hl.vds.read_vds(args.base_cohort_vds_path_out)
            vds_check_sample_count = vds_check.variant_data.count_cols() # Action
            vds_check_vd_partitions = vds_check.variant_data.n_partitions()
            vds_check_rd_partitions = vds_check.reference_data.n_partitions()
            print(f"Optimized checkpoint verified. Sample count: {vds_check_sample_count}. Partitions: vd={vds_check_vd_partitions}, rd={vds_check_rd_partitions}")
            
            if vds_check_sample_count != num_samples_in_vds:
                 print(f"WARNING: Sample count mismatch in final checkpoint! Expected {num_samples_in_vds}, got {vds_check_sample_count}")
            if not (target_vd_partitions * 0.8 < vds_check_vd_partitions < target_vd_partitions * 1.2):
                 print(f"WARNING: variant_data partition count ({vds_check_vd_partitions}) differs significantly from target ({target_vd_partitions}).")
            if not (dynamic_partitions * 0.8 < vds_check_rd_partitions < dynamic_partitions * 1.2):
                 print(f"WARNING: reference_data partition count ({vds_check_rd_partitions}) differs significantly from target ({dynamic_partitions}).")
            del vds_check
        except Exception as e:
            print(f"ERROR: Failed to write or verify final OPTIMIZED Base Cohort VDS checkpoint: {e}")
            # The user feedback included a more specific delete here, but sys.exit(1) is fine.
            # if not delete_gcs_path(args.base_cohort_vds_path_out, recursive=True, project_id_for_billing=args.google_billing_project):
            #      print(f"WARNING: Failed to delete VDS checkpoint at {args.base_cohort_vds_path_out} after write/verify failure.")
            sys.exit(1) 

        print("--- Base VDS Generation (Optimized) Finished ---") # This print is from user feedback
        base_cohort_vds = vds_final_optimized # Assign the successfully generated VDS
    # -------- END INSANELY MASSIVE SPEEDUP SECTION -------- (This comment is from original issue, good to keep conceptually)

    # Final check on base_cohort_vds before concluding (from user feedback)
    if base_cohort_vds is None: # Should not happen if generation was successful
        print(f"FATAL ERROR: base_cohort_vds is None at the end of processing despite generation attempt. Cannot proceed.")
        sys.exit(1)

    final_sample_count = base_cohort_vds.variant_data.count_cols() # Action
    if final_sample_count == 0:
        print(f"FATAL ERROR: Final Base Cohort VDS at {args.base_cohort_vds_path_out} has 0 samples.")
        if hail_path_exists(args.base_cohort_vds_path_out, project_id_for_billing=args.google_billing_project):
            delete_gcs_path(args.base_cohort_vds_path_out, project_id_for_billing=args.google_billing_project, recursive=True)
        sys.exit(1)
    print(f"Base Cohort VDS at {args.base_cohort_vds_path_out} is ready with {final_sample_count} samples.\n")

    # Save the sample ID list used for the VDS (whether downsampled or full WGS+EHR)
    # (Logic from user feedback)
    df_to_save_ids = None
    if 'final_ids_for_vds_df' not in locals() or final_ids_for_vds_df is None or final_ids_for_vds_df.empty:
        print("INFO: 'final_ids_for_vds_df' not directly available. Extracting IDs from final VDS for saving.")
        ids_from_vds_ht = base_cohort_vds.variant_data.cols().select('s')
        df_to_save_ids = ids_from_vds_ht.to_pandas().rename(columns={'s': 'person_id'})
    else:
        df_to_save_ids = final_ids_for_vds_df.rename(columns={'s': 'person_id'})

    save_sample_ids_to_gcs(df_to_save_ids, args.wgs_ehr_ids_gcs_path_out, fs) # fs is defined at start of main

    print(f"Script completed successfully.")
    print(f"Output Base VDS path: {args.base_cohort_vds_path_out}")
    print(f"Output Sample ID list path: {args.wgs_ehr_ids_gcs_path_out}")

if __name__ == "__main__":
    main()