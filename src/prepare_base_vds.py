import argparse
import datetime
import os
import sys
import time
import pandas as pd
import hail as hl
from utils import (
    init_hail, gcs_path_exists, hail_path_exists, delete_gcs_path,
    cache_result, get_gcs_fs, get_cache_dir
)

def parse_args():
    """Parses command-line arguments for the VDS preparation script."""
    parser = argparse.ArgumentParser(description="Prepare Base Cohort VDS for PRS Analysis.")
    parser.add_argument("--google_billing_project", required=True, help="Google Cloud Project ID for billing and GCS access.")
    parser.add_argument("--workspace_cdr", required=True, help="Workspace CDR (e.g., fc-aou-cdr-prod-ct.CYYYYQQRR), used for querying WGS+EHR samples.")
    parser.add_argument("--run_timestamp", required=True, help="Run timestamp (YYYYMMDD_HHMMSS) for Hail logging.")
    parser.add_argument("--gcs_temp_dir", required=True, help="GCS base directory for stable intermediate checkpoints (VDS, ID lists).") # This argument seems unused in the provided code. Consider removing if not needed.
    parser.add_argument("--gcs_hail_temp_dir", required=True, help="GCS temporary directory specifically for Hail shuffle/intermediate operations.")
    parser.add_argument("--wgs_vds_path", required=True, help="GCS path to the full input WGS VDS.")
    parser.add_argument("--flagged_samples_gcs_path", required=True, help="GCS path to the TSV file containing flagged sample IDs.") # Ensure this is the correct path. The prompt mentioned it was used twice for load_excluded_samples_ht.
    parser.add_argument("--base_cohort_vds_path_out", required=True, help="GCS output path for the prepared target cohort VDS checkpoint.") # Changed "base" to "target" for clarity
    parser.add_argument("--wgs_ehr_ids_gcs_path_out", required=True, help="GCS output path for the final list of sample IDs included in the target VDS (CSV format, 'person_id' column).") # Changed "base" to "target"

    # Arguments related to downsampling logic
    parser.add_argument("--enable_downsampling_for_vds", action='store_true', help="Enable case/control downsampling during VDS generation.") # This is critical for creating final_ids_for_vds_df
    parser.add_argument("--phenotype_cases_gcs_path_input", required=True, help="GCS path to the input CSV containing phenotype case definitions (requires 's' and 'phenotype_status' columns), generated by the fetch_phenotypes step. Used only if --enable_downsampling_for_vds is set.")
    parser.add_argument("--target_phenotype_name", required=True, help="Target phenotype name, primarily used for logging during downsampling.")
    parser.add_argument("--n_cases_downsample", type=int, default=500, help="Target number of cases for downsampling (if enabled).")
    parser.add_argument("--n_controls_downsample", type=int, default=500, help="Target number of controls for downsampling (if enabled).")
    parser.add_argument("--downsampling_random_state", type=int, default=2025, help="Random state seed for reproducible downsampling (if enabled).")
    return parser.parse_args()

# --- VDS Preparation Functions ---

# KEEP load_full_vds, load_excluded_samples_ht, filter_samples_by_exclusion_list,
# get_wgs_ehr_samples_df, save_sample_ids_to_gcs, load_phenotype_cases_from_csv.
# REMOVE filter_vds_to_sample_list() as its logic is integrated into main().

def load_full_vds(path: str, fs_gcs) -> hl.vds.VariantDataset: # fs_gcs seems unused here, Hail handles GCS internally. Consider removing if not used by other logic in this function.
    """Loads the full WGS VDS from the specified path."""
    print(f"Loading full WGS VariantDataset from: {path}")
    try:
        # Hail's read_vds uses its own GCS connector configuration.
        vds = hl.vds.read_vds(path)

        # Perform a lightweight check to ensure the VDS is readable and has columns.
        if not vds.variant_data.cols().take(1): # .cols().take(1) is cheaper than .count_cols()
            print(f"FATAL ERROR: Loaded VDS from {path} appears to have 0 samples.")
            sys.exit(1)

        print("VDS successfully loaded (initial column presence check passed).")
        print(f"Initial VDS variant_data n_partitions: {vds.variant_data.n_partitions()}")
        print(f"Initial VDS reference_data n_partitions: {vds.reference_data.n_partitions()}")
        try:
            print(f"Initial VDS variant_data entry schema: {vds.variant_data.entry.dtype}\n")
        except Exception as schema_e:
            print(f"Could not retrieve VDS variant_data entry schema: {schema_e}\n")
        return vds
    except Exception as e:
        if "requester_pays" in str(e).lower(): # Specific check for common GCS issue
            print(f"FATAL ERROR: Failed to load VDS from {path} due to requester pays issue. Check Hail/Spark GCS connector config. Error: {e}")
        else:
            print(f"FATAL ERROR: Failed to load or verify VDS from {path}: {e}")
        sys.exit(1)

def load_excluded_samples_ht(path_primary: str, path_secondary_or_default: str, fs_gcs) -> hl.Table | None: # fs_gcs used by gcs_path_exists
    """
    Loads the table of flagged/excluded samples.
    Uses path_primary. If it fails or path_primary is not found, it can check path_secondary_or_default.
    The prompt had args.flagged_samples_gcs_path passed twice, this function now reflects that structure if needed,
    or path_secondary_or_default can be the same as path_primary if only one path is relevant.
    """
    # Determine which path to actually use based on existence, prioritizing primary
    actual_path_to_load = None
    if gcs_path_exists(path_primary): # Removed fs=fs_gcs
        actual_path_to_load = path_primary
    elif path_secondary_or_default and gcs_path_exists(path_secondary_or_default): # Removed fs=fs_gcs
        print(f"Primary flagged samples path {path_primary} not found. Using secondary/default: {path_secondary_or_default}")
        actual_path_to_load = path_secondary_or_default
    else:
        error_message = f"ERROR: Flagged samples file not found or accessible at primary path {path_primary}"
        if path_secondary_or_default and path_primary != path_secondary_or_default:
            error_message += f" or secondary/default path {path_secondary_or_default}."
        else:
            error_message += "."
        print(error_message)
        print("CRITICAL INFO: Proceeding without relatedness/flagged sample filtering. This may impact analysis results.")
        return None

    print(f"Importing flagged (related or excluded) samples from: {actual_path_to_load}")
    try:
        ht = hl.import_table(actual_path_to_load, key='sample_id', impute=True) # sample_id is the typical key
        count = ht.count()
        print(f"Flagged samples loaded. Count: {count}\n")
        if count == 0:
            print("WARNING: Flagged samples table is empty.")
        return ht
    except Exception as e:
        print(f"ERROR: Failed to load flagged samples from {actual_path_to_load}: {e}")
        print("Proceeding without relatedness filtering due to load error.")
        return None

def filter_samples_by_exclusion_list(vds: hl.vds.VariantDataset, excluded_ht: hl.Table | None) -> hl.vds.VariantDataset:
    """Filters samples present in the excluded_ht from the VDS."""
    if excluded_ht is None:
        print("Skipping relatedness/flagged sample filtering as excluded table is not available.")
        return vds

    print("Filtering flagged samples out of the VDS...")
    try:
        n_before = vds.variant_data.count_cols()
        # Ensure the excluded_ht is keyed by 's' (string) to match VDS sample ID 's'
        if 's' not in excluded_ht.key or excluded_ht.key.s.dtype != hl.tstr:
            print("Re-keying excluded_ht from 'sample_id' (or current key) to 's' (string type).")
            # Assumes original key was 'sample_id' or can be accessed as such.
            # If 'sample_id' field doesn't exist, this will fail. Ensure input TSV has 'sample_id'.
            if 'sample_id' not in excluded_ht.row:
                 print("FATAL ERROR: 'sample_id' field not found in excluded_ht for re-keying. Ensure input TSV has a 'sample_id' column.")
                 sys.exit(1)
            excluded_ht = excluded_ht.annotate(s=hl.str(excluded_ht.sample_id)).key_by('s')

        # Filter cols (samples) in both variant and reference data
        # Use hl.is_missing to keep samples NOT in excluded_ht (anti-join logic)
        cleaned_variant_data = vds.variant_data.filter_cols(
            hl.is_missing(excluded_ht[vds.variant_data.s])
        )
        cleaned_ref_data = vds.reference_data.filter_cols(
            hl.is_missing(excluded_ht[vds.reference_data.s])
        )
        cleaned_vds = hl.vds.VariantDataset(cleaned_ref_data, cleaned_variant_data)
        
        n_after = cleaned_vds.variant_data.count_cols()
        print(f"Samples filtered. Count before: {n_before}, Count after: {n_after}. Removed: {n_before - n_after}\n")
        if n_after == 0:
            print("FATAL ERROR: Filtering removed all samples from the VDS.")
            sys.exit(1)
        return cleaned_vds
    except Exception as e:
        print(f"ERROR: Failed during sample filtering by exclusion list: {e}")
        print("Exiting due to failure during sample filtering.")
        sys.exit(1)

@cache_result("wgs_ehr_samples_df") # Decorator from utils
def get_wgs_ehr_samples_df(cdr_env_var_value: str, prs_id=None) -> pd.DataFrame: # prs_id seems unused
    """Fetches person_ids with both WGS and EHR data from BigQuery."""
    get_cache_dir() # Ensure local cache directory exists
    print("Retrieving WGS+EHR samples from BigQuery...")
    if not cdr_env_var_value:
        print("FATAL ERROR: Workspace CDR value not provided.")
        sys.exit(1)

    wgs_ehr_query = f"""
    SELECT person_id
    FROM `{cdr_env_var_value}.person`
    WHERE person_id IN (
        SELECT DISTINCT person_id
        FROM `{cdr_env_var_value}.cb_search_person`
        WHERE has_ehr_data = 1
        AND has_whole_genome_variant = 1
    )
    """
    print("Executing BQ query for WGS+EHR samples.")
    try:
        df = pd.read_gbq(wgs_ehr_query, dialect='standard', progress_bar_type=None) # Ensure BigQuery client is configured
        n_found = df['person_id'].nunique()
        print(f"WGS+EHR query completed. Found {n_found} unique persons.\n")
        if df.empty:
            print("FATAL ERROR: No samples found with both WGS and EHR data from BigQuery.")
            sys.exit(1)
        return df
    except Exception as e:
        print(f"FATAL ERROR: Failed to query BigQuery for WGS+EHR samples: {e}")
        sys.exit(1)

def save_sample_ids_to_gcs(df: pd.DataFrame, gcs_path: str, fs_gcs):
    """Saves the DataFrame of sample IDs (expected 'person_id' column) to GCS."""
    if df is None or df.empty:
        print("FATAL ERROR: Cannot save sample IDs DataFrame as it is None or empty.")
        sys.exit(1)
    if 'person_id' not in df.columns:
        print(f"FATAL ERROR: DataFrame to be saved to {gcs_path} must contain a 'person_id' column. Found: {df.columns.tolist()}")
        sys.exit(1)

    print(f"Storing sample IDs (column: 'person_id') to GCS: {gcs_path}")
    try:
        parent_dir = os.path.dirname(gcs_path)
        if not gcs_path_exists(parent_dir): # Removed fs=fs_gcs
             print(f"Creating GCS directory: {parent_dir}")
             # Assuming fs_gcs is the GCSFileSystem instance initialized in main and passed to this function
             fs_gcs.mkdirs(parent_dir) # exist_ok=True is default for GCSFS.mkdirs for gcsfs, ensure your util matches
             
        with fs_gcs.open(gcs_path, 'w') as f:
            df[['person_id']].to_csv(f, index=False) # Save only the 'person_id' column with header
        print("Sample IDs saved successfully.\n")
    except Exception as e:
        print(f"FATAL ERROR: Failed to save sample IDs to {gcs_path}: {e}")
        sys.exit(1)

# REMOVED filter_vds_to_sample_list function - its logic is now in main()

def load_phenotype_cases_from_csv(gcs_path: str, fs_gcs) -> pd.DataFrame:
    """Loads the phenotype case definitions from the specified GCS CSV file."""
    print(f"Loading phenotype case data from CSV: {gcs_path}")
    if not gcs_path_exists(gcs_path): # Removed fs=fs_gcs
        print(f"FATAL ERROR: Phenotype cases CSV file not found at {gcs_path}")
        sys.exit(1)
    try:
        with fs_gcs.open(gcs_path, 'r') as f:
            df = pd.read_csv(f)
        
        required_cols = ['s', 'phenotype_status'] # 's' for sample ID, 'phenotype_status' for case (1) / control (0)
        if not all(col in df.columns for col in required_cols):
             print(f"FATAL ERROR: Phenotype cases CSV {gcs_path} must contain columns: {required_cols}. Found: {df.columns.tolist()}")
             sys.exit(1)
             
        df['s'] = df['s'].astype(str) # Ensure sample ID is string for merging
        df['phenotype_status'] = df['phenotype_status'].astype(int)
        
        print(f"Successfully loaded {len(df)} entries from phenotype cases CSV.")
        # The original function returned only cases. The prompt implies the merge happens on all WGS+EHR,
        # then phenotype_status is used. So, returning the full df and letting main handle status is fine.
        # If only cases were needed, one could filter here: df = df[df['phenotype_status'] == 1]
        # For now, returning all entries as the downsampling logic handles case/control separation.
        return df # Return the full DataFrame, downsampling logic will use 'phenotype_status'
        
    except Exception as e:
        print(f"FATAL ERROR loading phenotype cases CSV from {gcs_path}: {e}")
        sys.exit(1)

# --- Main Script Execution ---

def main():
    args = parse_args()
    get_cache_dir() # Ensure local cache directory exists

    fs = get_gcs_fs(project_id_for_billing=args.google_billing_project)
    init_hail(
        gcs_hail_temp_dir=args.gcs_hail_temp_dir,
        log_suffix=args.run_timestamp
    )
    # Removed dynamic_partitions and hl.default_n_partitions setting here,
    # as global repartitioning of variant_data is removed.
    # If other Hail operations benefit, it could be reinstated, but the main driver was VDS write.

    TARGET_COHORT_VDS_PATH_OUT = args.base_cohort_vds_path_out # Using this as the primary output path name
    TARGET_COHORT_IDS_GCS_PATH_OUT = args.wgs_ehr_ids_gcs_path_out

    vds_for_target_cohort = None # This will hold the final VDS

    # --- Check for Existing Target Cohort VDS Checkpoint ---
    if hail_path_exists(TARGET_COHORT_VDS_PATH_OUT): # Uses utils.hail_path_exists
        print(f"[CHECKPOINT] Found potential Target Cohort VDS at: {TARGET_COHORT_VDS_PATH_OUT}")
        try:
            print("Attempting to read Target Cohort VDS checkpoint...")
            vds_for_target_cohort = hl.vds.read_vds(TARGET_COHORT_VDS_PATH_OUT)
            n_samples = vds_for_target_cohort.variant_data.count_cols()
            # n_variants = vds_for_target_cohort.variant_data.count_rows() # count_rows() can be expensive
            print(f"Sanity check on loaded VDS: Found {n_samples} samples.") # Only check samples for speed
            if n_samples > 0:
                print("[CHECKPOINT HIT] Successfully loaded Target Cohort VDS checkpoint.\n")
            else: 
                print("[CHECKPOINT CORRUPTED] Loaded VDS has 0 samples. Assuming invalid.")
                vds_for_target_cohort = None 
                if not delete_gcs_path(TARGET_COHORT_VDS_PATH_OUT, recursive=True): # Removed fs=fs
                     print(f"WARNING: Failed to delete corrupted VDS checkpoint at {TARGET_COHORT_VDS_PATH_OUT}")
        except Exception as e:
            print(f"[CHECKPOINT CORRUPTED] Failed to read VDS from {TARGET_COHORT_VDS_PATH_OUT}. Error: {str(e)}")
            vds_for_target_cohort = None # Ensure it's None on failure
            # Attempt deletion only if clearly a VDS format issue, otherwise could be GCS permissions etc.
            if "Not a VDS" in str(e) or "metadata.json.gz is missing" in str(e):
                 if not delete_gcs_path(TARGET_COHORT_VDS_PATH_OUT, recursive=True): # Removed fs=fs
                    print(f"WARNING: Failed to delete VDS checkpoint at {TARGET_COHORT_VDS_PATH_OUT} after read failure.")
            else:
                print(f"Error reading VDS checkpoint was not typical corruption. Investigate {TARGET_COHORT_VDS_PATH_OUT} before deleting.")


    if vds_for_target_cohort is None:
        print(f"[CHECKPOINT MISS or CORRUPTED] Target Cohort VDS needs to be generated at {TARGET_COHORT_VDS_PATH_OUT}.")
        print("--- Starting Target Cohort VDS Generation ---")

        # 1. Load Full VDS
        full_vds = load_full_vds(args.wgs_vds_path, fs) # fs might be optional for load_full_vds

        # 2. Load Excluded Samples List (flagged/related)
        # The prompt used args.flagged_samples_gcs_path twice. Assuming it might refer to two different paths or a primary/fallback.
        # load_excluded_samples_ht is modified to potentially check a secondary path if primary not found.
        # If only one path, pass it as both arguments or adjust load_excluded_samples_ht.
        excluded_ht = load_excluded_samples_ht(args.flagged_samples_gcs_path, args.flagged_samples_gcs_path, fs)


        # 3. Filter Excluded Samples from Full VDS
        vds_after_exclusion_filter = filter_samples_by_exclusion_list(full_vds, excluded_ht)
        del full_vds, excluded_ht # Free memory

        # 4. Determine the ~1000 Target Sample IDs (Pandas logic - KEEP THIS AS IS from prompt)
        all_wgs_ehr_individuals_df = get_wgs_ehr_samples_df(args.workspace_cdr)
        all_wgs_ehr_individuals_df = all_wgs_ehr_individuals_df.rename(columns={'person_id': 's'}) # Rename to 's' for consistency
        all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str) # Ensure 's' is string

        final_ids_for_vds_df = None # DataFrame with 's' column
        if args.enable_downsampling_for_vds:
            print(f"--- Downsampling enabled for VDS generation ---")
            print(f"Target: ~{args.n_cases_downsample} cases / ~{args.n_controls_downsample} controls for phenotype '{args.target_phenotype_name}'")
            
            phenotype_data_df = load_phenotype_cases_from_csv(args.phenotype_cases_gcs_path_input, fs)
            # Ensure 's' column in phenotype_data_df is string type for merging
            phenotype_data_df['s'] = phenotype_data_df['s'].astype(str)

            # Merge WGS+EHR individuals with phenotype data
            merged_cohort_phenotype_df = pd.merge(
                all_wgs_ehr_individuals_df[['s']], 
                phenotype_data_df[['s', 'phenotype_status']], # phenotype_data_df now has 's' and 'phenotype_status'
                on='s', how='left' # Keep all WGS+EHR individuals
            )
            # Individuals not in phenotype_data_df (or without status) become controls (phenotype_status=0)
            merged_cohort_phenotype_df['phenotype_status'] = merged_cohort_phenotype_df['phenotype_status'].fillna(0).astype(int)
            
            cases_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 1]
            controls_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 0]
            print(f"Available cases in WGS+EHR for downsampling: {len(cases_in_cohort_df)}")
            print(f"Available controls in WGS+EHR for downsampling: {len(controls_in_cohort_df)}")
            
            num_cases_to_sample = min(args.n_cases_downsample, len(cases_in_cohort_df))
            # Initialize empty df in case num_cases_to_sample is 0 (or len(cases_in_cohort_df) is 0)
            sampled_cases_df = pd.DataFrame(columns=['s', 'phenotype_status']) 
            if num_cases_to_sample > 0:
                sampled_cases_df = cases_in_cohort_df.sample(n=num_cases_to_sample, random_state=args.downsampling_random_state, replace=False)
            print(f"Sampled {len(sampled_cases_df)} cases.")

            num_controls_to_sample = min(args.n_controls_downsample, len(controls_in_cohort_df))
            sampled_controls_df = pd.DataFrame(columns=['s', 'phenotype_status']) # Initialize empty
            if num_controls_to_sample > 0:
                sampled_controls_df = controls_in_cohort_df.sample(n=num_controls_to_sample, random_state=args.downsampling_random_state, replace=False)
            print(f"Sampled {len(sampled_controls_df)} controls.")

            final_ids_for_vds_df = pd.concat([sampled_cases_df, sampled_controls_df], ignore_index=True)[['s']].drop_duplicates(subset=['s'])
            print(f"Total unique individuals in downsampled cohort for VDS generation: {len(final_ids_for_vds_df)}")
            
            if final_ids_for_vds_df.empty:
                print("FATAL ERROR: Downsampling resulted in an empty cohort. Check N_cases/N_controls, available data, and phenotype CSV.")
                sys.exit(1)
            del phenotype_data_df, merged_cohort_phenotype_df, cases_in_cohort_df, controls_in_cohort_df, sampled_cases_df, sampled_controls_df
        else:
            print("--- Using full WGS+EHR cohort (downsampling disabled) ---") 
            final_ids_for_vds_df = all_wgs_ehr_individuals_df[['s']] # Use the 's' column
        
        if final_ids_for_vds_df is None or final_ids_for_vds_df.empty: # Should be caught by previous empty check if downsampling enabled
             print("FATAL ERROR: No target sample IDs determined (final_ids_for_vds_df is None or empty).")
             sys.exit(1)
        
        del all_wgs_ehr_individuals_df # Free memory

        # 5. Filter VDS to the Target ~1000 Samples using Hail (NEW LOGIC)
        print(f"Converting {len(final_ids_for_vds_df)} target sample IDs from Pandas to HailTable...")
        target_samples_ht = hl.Table.from_pandas(final_ids_for_vds_df).key_by('s') # Key by 's' to match VDS

        print(f"Filtering VDS (after flagged/related removal) to the {target_samples_ht.count()} target samples...")
        # Ensure vds_after_exclusion_filter.variant_data.s is string, matching target_samples_ht.s
        # This should be guaranteed by filter_samples_by_exclusion_list and load_full_vds if they ensure 's' is string.
        # If there's a type mismatch error here, ensure 's' is consistently string type.
        vd_target_cohort_filtered = vds_after_exclusion_filter.variant_data.filter_cols(
            hl.is_defined(target_samples_ht[vds_after_exclusion_filter.variant_data.s])
        )
        rd_target_cohort_filtered = vds_after_exclusion_filter.reference_data.filter_cols(
            hl.is_defined(target_samples_ht[vds_after_exclusion_filter.reference_data.s])
        )
        vds_target_cohort = hl.vds.VariantDataset(rd_target_cohort_filtered, vd_target_cohort_filtered)
        
        # Free up memory immediately
        del vds_after_exclusion_filter, rd_target_cohort_filtered, vd_target_cohort_filtered, target_samples_ht 
        
        num_samples_in_vds = vds_target_cohort.variant_data.count_cols() # Capture count before truncate
        print(f"VDS filtered to target cohort. Samples: {num_samples_in_vds}. Variant_data row partitions: {vds_target_cohort.variant_data.n_partitions()}")
        
        if num_samples_in_vds == 0:
            print("FATAL ERROR: Filtering VDS to target sample list resulted in 0 samples remaining. Check input sample list and VDS contents.")
            sys.exit(1)
        # Also, compare num_samples_in_vds with len(final_ids_for_vds_df) as a sanity check
        if num_samples_in_vds != len(final_ids_for_vds_df):
            print(f"WARNING: Number of samples in VDS ({num_samples_in_vds}) does not match expected from Pandas list ({len(final_ids_for_vds_df)}). Some IDs may not have been in the VDS.")


        # 6. **NEW CRITICAL STEP**: Optimize reference_data for the Target Cohort VDS
        # This step uses hl.vds.truncate_reference_blocks to cap the length of reference blocks.
        # As per Hail 0.2.x documentation (and verified by source code provided by user):
        # - Function: hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)
        # - Purpose: Caps reference blocks at a maximum length to permit faster interval filtering later on.
        #   This is crucial for efficient querying, especially when using hl.vds.filter_intervals with split_reference_blocks=True
        #   (as done in process_prs_model.py) and for the hl.vds.to_dense_mt step.
        # - Parameter: `max_ref_block_base_pairs` specifies the maximum size of reference blocks in base pairs.
        # The All of Us documentation also highlights the importance of VDS optimizations for large datasets.
        MAX_REF_BLOCK_LENGTH = 10000  # e.g., 10kb. This is a tunable parameter.
        TRUNCATED_VDS_CHECKPOINT_PATH = args.base_cohort_vds_path_out.replace(".vds", "_truncated_checkpoint.vds") # Define a temp path

        print(f"Truncating reference blocks to max length {MAX_REF_BLOCK_LENGTH} bp for the target cohort VDS...")
        vds_truncated_intermediate = hl.vds.truncate_reference_blocks( # Assign to a new intermediate variable first
            vds_target_cohort, 
            max_ref_block_base_pairs=MAX_REF_BLOCK_LENGTH # Correct parameter for Hail 0.2.x
        )
        del vds_target_cohort 
        
        print(f"Checkpointing truncated VDS to: {TRUNCATED_VDS_CHECKPOINT_PATH}")
        try:
            vds_truncated_intermediate.write(TRUNCATED_VDS_CHECKPOINT_PATH, overwrite=True)
            print("Reading back checkpointed truncated VDS...")
            vds_final_prepared = hl.vds.read_vds(TRUNCATED_VDS_CHECKPOINT_PATH)
            del vds_truncated_intermediate # Free memory
        except Exception as e:
            print(f"ERROR during checkpointing of truncated VDS: {e}")
            # If checkpointing fails, attempt to use the in-memory VDS, but it might be risky
            # Or, exit. For now, let's try to proceed if read_vds failed but write might have partially succeeded.
            # A more robust solution would be to ensure cleanup or handle more gracefully.
            # For this change, the main goal is to materialize the result of truncate_reference_blocks.
            # If the write itself fails, that's the point of failure. If read fails, something is very wrong.
            # Let's assume if write succeeds, read should typically succeed.
            # If write fails, the script would likely exit here or in the next step.
            # Re-assigning to vds_final_prepared to ensure it's defined, though it might be None or problematic.
            # A better approach might be to sys.exit(1) if checkpointing fails.
            print("Attempting to use non-checkpointed VDS due to checkpoint error. This may be unstable.")
            vds_final_prepared = vds_truncated_intermediate # Fallback, though risky
            # Consider adding sys.exit(1) here if checkpoint is critical
            # For now, just logging and trying to proceed. A more robust approach might be to sys.exit here.
            # sys.exit(f"FATAL: Failed to write/read truncated VDS checkpoint at {TRUNCATED_VDS_CHECKPOINT_PATH}. Error: {e}")

        print(f"Reference blocks processed (checkpointed). Resulting VDS variant_data partitions: {vds_final_prepared.variant_data.n_partitions()}, Reference_data partitions: {vds_final_prepared.reference_data.n_partitions()}")


        # 7. **REMOVED GLOBAL REPARTITIONING OF VARIANT_DATA**
        # The VDS `vds_final_prepared` will retain the partitioning from the `truncate_reference_blocks` step,
        # or ultimately from the `vds_after_exclusion_filter` if truncate_reference_blocks doesn't change variant_data partitioning.
        # No explicit repartitioning of variant_data is done here. Reference_data might be repartitioned by truncate_reference_blocks.

        # 8. Write the Prepared Target Cohort VDS (this is now vds_final_prepared)
        print(f"Writing prepared Target Cohort VDS checkpoint to: {TARGET_COHORT_VDS_PATH_OUT}")
        try:
            # Overwrite is True because if this block is running, either no checkpoint existed or it was deemed corrupted and deleted.
            vds_final_prepared.write(TARGET_COHORT_VDS_PATH_OUT, overwrite=True) 
            print("Target Cohort VDS checkpoint successfully written.")
            
            print("Verifying written checkpoint...")
            vds_check = hl.vds.read_vds(TARGET_COHORT_VDS_PATH_OUT)
            vds_check_sample_count = vds_check.variant_data.count_cols()
            print(f"Checkpoint verified. Sample count: {vds_check_sample_count}")
            
            # Compare with expected count from before write (num_samples_in_vds)
            if vds_check_sample_count != num_samples_in_vds: # num_samples_in_vds was count before truncate_reference_blocks
                 print(f"WARNING: Sample count mismatch in checkpoint! Expected {num_samples_in_vds}, got {vds_check_sample_count}. This should not happen due to truncate_reference_blocks.")
            del vds_check # Clean up verification object
        except Exception as e:
            print(f"ERROR: Failed to write or verify Target Cohort VDS: {e}")
            # Attempt to delete if write failed partway, to prevent corrupted state for next run
            if not delete_gcs_path(TARGET_COHORT_VDS_PATH_OUT, recursive=True): # Removed fs=fs
                 print(f"WARNING: Failed to delete VDS checkpoint at {TARGET_COHORT_VDS_PATH_OUT} after write/verify failure.")
            sys.exit(1) # Exit if write fails
        
        vds_for_target_cohort = vds_final_prepared # Assign to the main variable used outside this block
        # Do not delete vds_final_prepared here as it's now assigned to vds_for_target_cohort
        print("--- Target Cohort VDS Generation Finished ---")

    # --- Final Check and Save Sample List ---
    if vds_for_target_cohort is None: # This should ideally be caught earlier if generation failed
        print(f"FATAL ERROR: vds_for_target_cohort is None. VDS generation or loading failed. Cannot proceed.")
        sys.exit(1)
    
    final_sample_count = vds_for_target_cohort.variant_data.count_cols()
    if final_sample_count == 0:
        print(f"FATAL ERROR: Final Target Cohort VDS at {TARGET_COHORT_VDS_PATH_OUT} has 0 samples.")
        # Consider deleting the empty VDS if it was written
        delete_gcs_path(TARGET_COHORT_VDS_PATH_OUT, recursive=True) # Removed fs=fs
        sys.exit(1)
    print(f"Target Cohort VDS at {TARGET_COHORT_VDS_PATH_OUT} is ready with {final_sample_count} samples.\n")

    # Save the list of sample IDs that were INTENDED for the VDS (from final_ids_for_vds_df)
    # This list is crucial for knowing the target cohort, even if some IDs were not found in the source VDS.
    if final_ids_for_vds_df is None or final_ids_for_vds_df.empty:
        print("FATAL ERROR: final_ids_for_vds_df is not available for saving sample list. This indicates a logic error in sample determination.")
        sys.exit(1) # Exit because final_ids_for_vds_df should always be populated if VDS generation was attempted.
        
    # Rename 's' column to 'person_id' before saving, as per requirements
    df_to_save_target_ids = final_ids_for_vds_df.rename(columns={'s': 'person_id'})
    save_sample_ids_to_gcs(df_to_save_target_ids, TARGET_COHORT_IDS_GCS_PATH_OUT, fs)

    print(f"Script completed successfully.")
    print(f"Output Target Cohort VDS path: {TARGET_COHORT_VDS_PATH_OUT}")
    print(f"Output Target Cohort Sample ID list path: {TARGET_COHORT_IDS_GCS_PATH_OUT}")

if __name__ == "__main__":
    main()