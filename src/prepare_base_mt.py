import argparse
import datetime
import os
import sys
import time
import pandas as pd
import hail as hl
from utils import (
    init_hail, gcs_path_exists, hail_path_exists, delete_gcs_path,
    cache_result, get_gcs_fs, get_cache_dir
)

def parse_args():
    """Parses command-line arguments for the MT preparation script."""
    parser = argparse.ArgumentParser(description="Prepare Base Cohort MatrixTable for PRS Analysis.")
    parser.add_argument("--google_billing_project", required=True, help="Google Cloud Project ID for billing and GCS access.")
    parser.add_argument("--workspace_cdr", required=True, help="Workspace CDR (e.g., fc-aou-cdr-prod-ct.CYYYYQQRR), used for querying WGS+EHR samples.")
    parser.add_argument("--run_timestamp", required=True, help="Run timestamp (YYYYMMDD_HHMMSS) for Hail logging.")
    parser.add_argument("--gcs_temp_dir", required=True, help="GCS base directory for stable intermediate checkpoints (MT, ID lists).")
    parser.add_argument("--gcs_hail_temp_dir", required=True, help="GCS temporary directory specifically for Hail shuffle/intermediate operations.")
    parser.add_argument("--aou_input_mt_path", required=True, help="GCS path to the input AoU dense Hail MatrixTable, e.g., ACAF split MT.")
    parser.add_argument("--flagged_samples_gcs_path", required=True, help="GCS path to the TSV file containing flagged sample IDs.")
    parser.add_argument("--base_cohort_mt_path_out", required=True, help="GCS output path for the prepared base cohort MatrixTable checkpoint.")
    parser.add_argument("--wgs_ehr_ids_gcs_path_out", required=True, help="GCS output path for the final list of sample IDs included in the base MT (CSV format, 'person_id' column).")

    # Arguments related to downsampling logic
    parser.add_argument("--enable_downsampling_for_mt", action='store_true', help="Enable case/control downsampling during MT generation.")
    parser.add_argument("--phenotype_cases_gcs_path_input", required=True, help="GCS path to the input CSV containing phenotype case definitions (requires 's' and 'phenotype_status' columns), generated by the fetch_phenotypes step. Used only if --enable_downsampling_for_mt is set.")
    parser.add_argument("--target_phenotype_name", required=True, help="Target phenotype name, primarily used for logging during downsampling.")
    parser.add_argument("--n_cases_downsample", type=int, default=500, help="Target number of cases for downsampling (if enabled).")
    parser.add_argument("--n_controls_downsample", type=int, default=500, help="Target number of controls for downsampling (if enabled).")
    parser.add_argument("--downsampling_random_state", type=int, default=2025, help="Random state seed for reproducible downsampling (if enabled).")
    return parser.parse_args()

# --- MT Preparation Functions ---

def load_aou_input_mt(path: str) -> hl.MatrixTable:
    """Loads the AoU input MatrixTable from the specified path."""
    print(f"Loading AoU input MatrixTable from: {path}")
    try:
        mt = hl.read_matrix_table(path)
        
        num_cols = mt.count_cols() # Action
        num_rows = mt.count_rows() # Action
        if num_cols == 0 or num_rows == 0:
            print(f"FATAL ERROR: Loaded MatrixTable from {path} appears to have 0 samples ({num_cols}) or 0 variants ({num_rows}).")
            sys.exit(1)

        print(f"INFO: Input AoU MatrixTable successfully loaded. Samples: {num_cols}, Variants: {num_rows}")
        print(f"INFO: Input MT n_partitions: {mt.n_partitions()}") # Action
        try:
            # print(f"Initial MT entry schema: {mt.entry.dtype}\n") # Reduced verbosity
            # print(f"Initial MT row schema: {mt.row.dtype}\n")
            # print(f"Initial MT col schema: {mt.col.dtype}\n")
        except Exception as schema_e:
            print(f"Could not retrieve MT entry schema: {schema_e}\n")
        return mt
    except Exception as e:
        if "requester_pays" in str(e).lower():
            print(f"FATAL ERROR: Failed to load MatrixTable from {path} due to requester pays issue. Check Hail/Spark GCS connector config. Error: {e}")
        else:
            print(f"FATAL ERROR: Failed to load or verify MatrixTable from {path}: {e}")
        sys.exit(1)

def load_excluded_samples_ht(path: str, flagged_samples_gcs_path_default: str, fs_gcs) -> hl.Table | None:
    """Loads the table of flagged/excluded samples, returning None if not found or on error."""
    print(f"Importing flagged (related or excluded) samples from: {path}")
    if not gcs_path_exists(path): 
        error_message = f"ERROR: Flagged samples file not found or accessible at {path}. "
        if path == flagged_samples_gcs_path_default:
            error_message += "This is the default All of Us path. The file might have moved or permissions changed."
        else:
            error_message += "This is a custom path. Please check the path and permissions."
        print(error_message)
        print("CRITICAL INFO: Proceeding without relatedness/flagged sample filtering. This may impact analysis results.")
        return None
    try:
        ht = hl.import_table(path, key='sample_id', impute=True)
        count = ht.count()
        print(f"Flagged samples loaded. Count: {count}\n")
        if count == 0:
            print("WARNING: Flagged samples table is empty.")
        return ht
    except Exception as e:
        print(f"ERROR: Failed to load flagged samples from {path}: {e}")
        print("Proceeding without relatedness filtering due to load error.")
        return None

def filter_samples_by_exclusion_list(
    mt: hl.MatrixTable, 
    excluded_ht: hl.Table | None, 
    id_column_name: str = 'sample_id'
) -> hl.MatrixTable:
    print(f"[FILTER_EXCLUSION] Starting sample filtering. Initial MT sample count: {mt.count_cols()}.") # Action
    
    if excluded_ht is None:
        print("[FILTER_EXCLUSION] Exclusion table is None. Returning original MT.")
        return mt

    initial_excluded_count = excluded_ht.count() # Action
    print(f"[FILTER_EXCLUSION] Exclusion table received with {initial_excluded_count} entries.")
    if initial_excluded_count == 0:
        print("[FILTER_EXCLUSION] Exclusion table is empty. Returning original MT.")
        return mt

    if id_column_name not in excluded_ht.row:
        msg = f"[FILTER_EXCLUSION] ERROR: ID column '{id_column_name}' not found in exclusion table. Columns: {list(excluded_ht.row.keys())}"
        print(msg)
        raise ValueError(msg)

    print(f"[FILTER_EXCLUSION] Preparing exclusion table. Original key: {list(excluded_ht.key.keys()) if excluded_ht.key is not None else 'None'}, target ID field for rekey: '{id_column_name}'.")
    
    temp_s_col = '_s_to_key_' # temp column name for string conversion
    
    if id_column_name == 's' and excluded_ht[id_column_name].dtype == hl.tstr:
        excluded_ht_rekeyed = excluded_ht.key_by('s')
        print(f"[FILTER_EXCLUSION] Exclusion table already has 's' key of type string.")
    elif id_column_name != 's' and excluded_ht[id_column_name].dtype == hl.tstr:
        excluded_ht_rekeyed = excluded_ht.key_by(id_column_name).rename({id_column_name: 's'})
        print(f"[FILTER_EXCLUSION] Exclusion table keyed by '{id_column_name}' (string) and key column renamed to 's'.")
    else: 
        print(f"[FILTER_EXCLUSION] Exclusion table field '{id_column_name}' (type {excluded_ht[id_column_name].dtype}) needs conversion/rekeying.")
        excluded_ht_rekeyed = excluded_ht.annotate(**{temp_s_col: hl.str(excluded_ht[id_column_name])})
        excluded_ht_rekeyed = excluded_ht_rekeyed.key_by(temp_s_col).rename({temp_s_col: 's'})
        print(f"[FILTER_EXCLUSION] Exclusion table converted to string, keyed by and key column renamed to 's'.")

    print(f"[FILTER_EXCLUSION] Schema of re-keyed exclusion table: {excluded_ht_rekeyed.row.dtype}, Key: {list(excluded_ht_rekeyed.key.keys())}")
    
    print(f"[FILTER_EXCLUSION] Persisting re-keyed exclusion table...")
    try:
        excluded_ht_rekeyed = excluded_ht_rekeyed.persist()
        count_after_persist = excluded_ht_rekeyed.count() # Action
        print(f"[FILTER_EXCLUSION] Re-keyed exclusion table persisted. Count: {count_after_persist}.")
        if initial_excluded_count != count_after_persist:
            print(f"[FILTER_EXCLUSION] WARNING: Count mismatch after persist. Before: {initial_excluded_count}, After: {count_after_persist}")
    except Exception as e:
        print(f"[FILTER_EXCLUSION] ERROR during persist: {e}. Continuing without persisted table.")

    mt_initial_cols = mt.count_cols() # Action
    print(f"[FILTER_EXCLUSION] Filtering MT (samples: {mt_initial_cols}, partitions: {mt.n_partitions()})...")
    try:
        if mt.s.dtype != hl.tstr:
            print(f"[FILTER_EXCLUSION] WARNING: MT 's' column is not string ({mt.s.dtype}). This might cause issues if exclusion keys are strings.")
        # The anti_join_cols variant:
        # filtered_mt = mt.filter_cols(hl.is_missing(excluded_ht_rekeyed[mt.s]))
        # Using anti_join_cols might be more explicit or performant in some Hail versions for this pattern.
        # However, filter_cols with is_missing is idiomatic. Let's stick to is_missing for now.
        filtered_mt = mt.filter_cols(hl.is_missing(excluded_ht_rekeyed[mt.s]))
        
        mt_cols_after_filter = filtered_mt.count_cols() # Action
        print(f"[FILTER_EXCLUSION] MT filtered. Samples remaining: {mt_cols_after_filter}. Partitions: {filtered_mt.n_partitions()}.")
    except Exception as e:
        print(f"[FILTER_EXCLUSION] ERROR filtering MT: {e}")
        raise

    print(f"[FILTER_EXCLUSION] New MatrixTable created. Final sample count: {mt_cols_after_filter}.")
    return filtered_mt

@cache_result("wgs_ehr_samples_df") 
def get_wgs_ehr_samples_df(cdr_env_var_value: str, prs_id=None) -> pd.DataFrame:
    get_cache_dir() 
    print("Retrieving WGS+EHR samples from BigQuery...")
    if not cdr_env_var_value:
        print(f"FATAL ERROR: Workspace CDR value not provided.")
        sys.exit(1)

    wgs_ehr_query = f"""
    SELECT person_id
    FROM `{cdr_env_var_value}.person`
    WHERE person_id IN (
        SELECT DISTINCT person_id
        FROM `{cdr_env_var_value}.cb_search_person`
        WHERE has_ehr_data = 1
        AND has_whole_genome_variant = 1
    )
    """
    print("Executing BQ query for WGS+EHR samples.")
    try:
        df = pd.read_gbq(wgs_ehr_query, dialect='standard', progress_bar_type=None)
        n_found = df['person_id'].nunique()
        print(f"WGS+EHR query completed. Found {n_found} unique persons.\n")
        if df.empty:
            print("FATAL ERROR: No samples found with both WGS and EHR data from BigQuery. Cannot proceed.")
            sys.exit(1)
        return df
    except Exception as e:
        print(f"FATAL ERROR: Failed to query BigQuery for WGS+EHR samples: {e}")
        sys.exit(1)

def save_sample_ids_to_gcs(df: pd.DataFrame, gcs_path: str, fs_gcs):
    if df is None or df.empty:
        print("FATAL ERROR: Cannot save sample IDs DataFrame as it is None or empty.")
        sys.exit(1)
    if 'person_id' not in df.columns:
        print(f"FATAL ERROR: DataFrame to be saved to {gcs_path} must contain a 'person_id' column.")
        sys.exit(1)

    print(f"Storing sample IDs (column: 'person_id') to GCS: {gcs_path}")
    try:
        parent_dir = os.path.dirname(gcs_path)
        if not gcs_path_exists(parent_dir): # fs_gcs is not passed here, gcs_path_exists will use global or re-init
             print(f"Creating GCS directory: {parent_dir}")
             # fs_gcs should be used if available and appropriate
             fs_gcs.mkdirs(parent_dir, exist_ok=True) # fs_gcs passed to parent function
             
        with fs_gcs.open(gcs_path, 'w') as f: # fs_gcs passed to parent function
            df[['person_id']].to_csv(f, index=False)
        print("Sample IDs saved successfully.\n")
    except Exception as e:
        print(f"FATAL ERROR: Failed to save sample IDs to {gcs_path}: {e}")
        sys.exit(1)

def filter_mt_to_sample_list(mt: hl.MatrixTable, sample_ids_gcs_path: str, fs_gcs) -> hl.MatrixTable:
    if mt is None:
        print("FATAL ERROR: MatrixTable is None, cannot filter to sample list.")
        sys.exit(1)
    if not sample_ids_gcs_path:
        print("FATAL ERROR: Sample IDs GCS path is None or empty, cannot filter MatrixTable.")
        sys.exit(1)

    print(f"Importing sample list from {sample_ids_gcs_path} for MT filtering...")
    try:
        if not gcs_path_exists(sample_ids_gcs_path): # fs_gcs not used here
            print(f"FATAL ERROR: Sample IDs file not found at {sample_ids_gcs_path}")
            sys.exit(1)

        ids_ht = hl.import_table(sample_ids_gcs_path, delimiter=',', key='person_id', types={'person_id': hl.tstr})
        
        if ids_ht.count() == 0: # Action
            print(f"FATAL ERROR: Imported sample ID HailTable from {sample_ids_gcs_path} is empty. Cannot filter MT.")
            sys.exit(1)

        # Check if MT 's' col key is already string. If not, warn.
        # AoU MTs are expected to have 's' as string.
        if mt.s.dtype != hl.tstr:
             print(f"WARNING: Input MT sample key 's' is of type {mt.s.dtype}, not string. Ensure this is compatible with sample ID list format.")
        
        ids_ht_keyed = ids_ht.key_by(s=ids_ht.person_id) # Ensure key is 's' for joining with MT

        print("Filtering MT to sample list...")
        # Use semi_join_cols to keep only columns present in ids_ht_keyed
        subset_mt = mt.semi_join_cols(ids_ht_keyed)

        n_after_filter = subset_mt.count_cols() # Action
        print(f"MT filtered to sample list. Final count: {n_after_filter}\n")
        if n_after_filter == 0:
             print(f"FATAL ERROR: Filtering MT to sample list from {sample_ids_gcs_path} resulted in 0 samples remaining.")
             sys.exit(1)
        return subset_mt
    except Exception as e:
        print(f"FATAL ERROR: Failed filtering MT to sample list: {e}")
        sys.exit(1)

def load_phenotype_cases_from_csv(gcs_path: str, fs_gcs) -> pd.DataFrame:
    print(f"Loading phenotype case data from CSV: {gcs_path}")
    # fs_gcs not used here for exists check
    if not gcs_path_exists(gcs_path):
        print(f"FATAL ERROR: Phenotype cases CSV file not found at {gcs_path}")
        sys.exit(1)
    try:
        with fs_gcs.open(gcs_path, 'r') as f: # fs_gcs used here
            df = pd.read_csv(f)
        
        required_cols = ['s', 'phenotype_status']
        if not all(col in df.columns for col in required_cols):
             print(f"FATAL ERROR: Phenotype cases CSV {gcs_path} must contain columns: {required_cols}. Found: {df.columns.tolist()}")
             sys.exit(1)
             
        df['s'] = df['s'].astype(str)
        df['phenotype_status'] = df['phenotype_status'].astype(int)
        
        print(f"Successfully loaded {len(df)} entries from phenotype cases CSV.")
        cases_df = df[df['phenotype_status'] == 1].copy()
        print(f"Identified {len(cases_df)} cases from the CSV.")
        return cases_df
        
    except Exception as e:
        print(f"FATAL ERROR loading phenotype cases CSV from {gcs_path}: {e}")
        sys.exit(1)

# --- Main Script Execution ---

def main():
    args = parse_args()
    get_cache_dir() 

    fs = get_gcs_fs(project_id_for_billing=args.google_billing_project)
    init_hail(
        gcs_hail_temp_dir=args.gcs_hail_temp_dir,
        log_suffix=args.run_timestamp 
    )
    dynamic_partitions = max(200, hl.spark_context().defaultParallelism * 4)
    try:
        hl.default_n_partitions(dynamic_partitions)
    except AttributeError:
        if hasattr(hl, "set_default_n_partitions"):
            hl.set_default_n_partitions(dynamic_partitions)

    base_cohort_mt = None 
    # OLD_TRUNCATED_VDS_CHECKPOINT_PATH is no longer relevant for MTs.

    if hail_path_exists(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project): 
        print(f"[CHECKPOINT] Found potential Base Cohort MT directory at: {args.base_cohort_mt_path_out}")
        try:
            print("Attempting to read MT checkpoint...")
            base_cohort_mt = hl.read_matrix_table(args.base_cohort_mt_path_out)
            n_samples = base_cohort_mt.count_cols() # Action
            n_variants = base_cohort_mt.count_rows() # Action
            print(f"Sanity check on loaded MT: Found {n_samples} samples and {n_variants} variants.")
            if n_samples > 0 and n_variants > 0: # Ensure MT is not empty
                print("[CHECKPOINT HIT] Successfully loaded and verified MT checkpoint.\n")
            else:
                print(f"[CHECKPOINT CORRUPTED/EMPTY] Loaded MT has 0 samples or 0 variants. Samples: {n_samples}, Variants: {n_variants}. Assuming invalid.")
                base_cohort_mt = None # Force regeneration
                print(f"Attempting to delete corrupted/empty MT checkpoint at {args.base_cohort_mt_path_out}")
                if not delete_gcs_path(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project, recursive=True):
                     print(f"WARNING: Failed to delete corrupted/empty MT checkpoint at {args.base_cohort_mt_path_out}")
        except Exception as e:
            error_message = str(e)
            print(f"[CHECKPOINT LOAD FAILED] Failed to read MT from {args.base_cohort_mt_path_out}. Error: {error_message}")
            base_cohort_mt = None # Force regeneration
            print(f"Attempting to delete MT checkpoint at {args.base_cohort_mt_path_out} due to read error.")
            if not delete_gcs_path(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project, recursive=True):
                 print(f"WARNING: Failed to delete MT checkpoint at {args.base_cohort_mt_path_out} after read error.")

    # If, after all checks, base_cohort_mt is still None, it means we need to generate it.
    if base_cohort_mt is None:
        print(f"[DECISION] Base Cohort MT needs to be generated/regenerated at {args.base_cohort_mt_path_out}.")

        # --- AGGRESSIVE AUTO-DELETION OF OLD/BAD STUFF BEFORE REGENERATION ---
        print("INFO: Preparing for MT regeneration. Ensuring target path is clear...")
        # Only the main output path needs cleaning for MTs, no old truncated format.
        if hail_path_exists(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project):
            print(f"INFO: Attempting to delete existing path: '{args.base_cohort_mt_path_out}'.")
            if not delete_gcs_path(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project, recursive=True):
                print(f"WARNING: Failed to delete '{args.base_cohort_mt_path_out}'. 'overwrite=True' will attempt to handle.")
        else:
            print(f"INFO: Path '{args.base_cohort_mt_path_out}' does not exist. No cleanup needed for this path.")
        # --- END AUTO-DELETION ---

        print("--- Starting Base MT Generation ---")

        input_aou_mt = load_aou_input_mt(args.aou_input_mt_path) # fs not needed here
        excluded_ht = load_excluded_samples_ht(args.flagged_samples_gcs_path, args.flagged_samples_gcs_path, fs)
        
        print("Starting process to filter MT by exclusion list...")
        initial_mt_sample_count = -1
        try:
            initial_mt_sample_count = input_aou_mt.count_cols() # Action
            print(f"Full MT sample count before exclusion filter: {initial_mt_sample_count}")
        except Exception as e:
            print(f"ERROR: Could not get initial MT sample count: {e}")
        
        cleaned_mt = filter_samples_by_exclusion_list(
            input_aou_mt, 
            excluded_ht, 
            id_column_name='sample_id' 
        )
        if cleaned_mt is not None:
            print("INFO: Persisting cleaned_mt to memory and disk.")
            cleaned_mt = cleaned_mt.persist('MEMORY_AND_DISK')
        else:
            print("WARNING: cleaned_mt is None after exclusion filtering. This is unexpected.")
            sys.exit("Exiting due to cleaned_mt being None.")
            
        del input_aou_mt, excluded_ht # Release original large MT and exclusion table

        # Logic to define final_ids_for_mt_df (either from WGS+EHR or downsampling)
        all_wgs_ehr_individuals_df = get_wgs_ehr_samples_df(args.workspace_cdr)
        all_wgs_ehr_individuals_df = all_wgs_ehr_individuals_df.rename(columns={'person_id': 's'})
        all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str)

        final_ids_for_mt_df = None # Initialize

        if args.enable_downsampling_for_mt:
            print(f"--- Downsampling enabled for MT generation ---")
            print(f"Target: ~{args.n_cases_downsample} cases / ~{args.n_controls_downsample} controls for phenotype '{args.target_phenotype_name}'")
            
            phenotype_cases_df = load_phenotype_cases_from_csv(args.phenotype_cases_gcs_path_input, fs)

            if 's' not in all_wgs_ehr_individuals_df.columns:
                print("FATAL ERROR: 's' column missing from all_wgs_ehr_individuals_df before downsampling merge.")
                sys.exit(1)
            all_wgs_ehr_individuals_df['s'] = all_wgs_ehr_individuals_df['s'].astype(str)
            
            if 's' not in phenotype_cases_df.columns:
                print("FATAL ERROR: 's' column missing from phenotype_cases_df before downsampling merge.")
                sys.exit(1)
            phenotype_cases_df['s'] = phenotype_cases_df['s'].astype(str)

            merged_cohort_phenotype_df = pd.merge(
                all_wgs_ehr_individuals_df[['s']], 
                phenotype_cases_df[['s', 'phenotype_status']], 
                on='s',
                how='left' 
            )
            merged_cohort_phenotype_df['phenotype_status'] = merged_cohort_phenotype_df['phenotype_status'].fillna(0).astype(int)
            
            print(f"Total WGS+EHR individuals available for downsampling: {len(merged_cohort_phenotype_df)}")
            print(f"Phenotype distribution within WGS+EHR cohort (1=case, 0=control/unknown):\n{merged_cohort_phenotype_df['phenotype_status'].value_counts(dropna=False)}")

            cases_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 1]
            controls_in_cohort_df = merged_cohort_phenotype_df[merged_cohort_phenotype_df['phenotype_status'] == 0]
            print(f"Available cases in WGS+EHR cohort: {len(cases_in_cohort_df)}")
            print(f"Available controls in WGS+EHR cohort: {len(controls_in_cohort_df)}")

            num_cases_to_sample = min(args.n_cases_downsample, len(cases_in_cohort_df))
            if len(cases_in_cohort_df) > 0 and num_cases_to_sample > 0 :
                 sampled_cases_df = cases_in_cohort_df.sample(n=num_cases_to_sample, random_state=args.downsampling_random_state, replace=False)
            else:
                 sampled_cases_df = pd.DataFrame(columns=['s', 'phenotype_status'])
            print(f"Sampled {len(sampled_cases_df)} cases.")

            num_controls_to_sample = min(args.n_controls_downsample, len(controls_in_cohort_df))
            if len(controls_in_cohort_df) > 0 and num_controls_to_sample > 0:
                sampled_controls_df = controls_in_cohort_df.sample(n=num_controls_to_sample, random_state=args.downsampling_random_state, replace=False)
            else:
                sampled_controls_df = pd.DataFrame(columns=['s', 'phenotype_status'])
            print(f"Sampled {len(sampled_controls_df)} controls.")

            final_ids_for_mt_df = pd.concat([sampled_cases_df, sampled_controls_df], ignore_index=True)[['s']]
            final_ids_for_mt_df = final_ids_for_mt_df.drop_duplicates(subset=['s'])
            print(f"Total unique individuals in downsampled cohort for MT generation: {len(final_ids_for_mt_df)}")
            
            if final_ids_for_mt_df.empty and (num_cases_to_sample > 0 or num_controls_to_sample > 0) :
                print("FATAL ERROR: Downsampling resulted in an empty cohort, but cases or controls were expected. Check N_cases/N_controls and available data.")
                sys.exit(1)
            elif final_ids_for_mt_df.empty:
                 print("INFO: Downsampling resulted in an empty cohort (no cases/controls requested or available).")

            del phenotype_cases_df, merged_cohort_phenotype_df, cases_in_cohort_df, controls_in_cohort_df, sampled_cases_df, sampled_controls_df
        else:
            print("--- Using full WGS+EHR cohort for MT generation (downsampling disabled) ---")
            final_ids_for_mt_df = all_wgs_ehr_individuals_df[['s']] 

        print(f"Converting {len(final_ids_for_mt_df)} target sample IDs to HailTable for MT filtering...")
        target_samples_ht = hl.Table.from_pandas(final_ids_for_mt_df).key_by('s')

        print(f"Filtering MT (after flagged/related removal) to the {target_samples_ht.count()} target samples...") # Action
        # Instead of filter_mt_to_sample_list, apply filter directly to cleaned_mt
        # This is now aliased as cleaned_mt_after_exclusion for clarity with new logging
        cleaned_mt_after_exclusion = cleaned_mt 
        current_base_mt = cleaned_mt_after_exclusion.filter_cols(hl.is_defined(target_samples_ht[cleaned_mt_after_exclusion.s]))
        
        # Unpersist cleaned_mt_after_exclusion (which is 'cleaned_mt') as it's been filtered into current_base_mt
        # and current_base_mt will be persisted after consolidation.
        if hasattr(cleaned_mt_after_exclusion, 'unpersist'):
            print("INFO: Unpersisting cleaned_mt_after_exclusion (original cleaned_mt).")
            cleaned_mt_after_exclusion.unpersist()
        del cleaned_mt, cleaned_mt_after_exclusion, target_samples_ht # Explicitly delete to free memory references

        # --- NEW CONSOLIDATION STEP ---
        print(f"INFO: current_base_mt defined by all sample filters. Attempting to consolidate its partitioning BEFORE first major computation.")
        consolidate_target_partitions = dynamic_partitions # Defined at the start of main()
        print(f"INFO: Consolidating current_base_mt to approximately {consolidate_target_partitions} partitions with shuffle=True.")
        current_base_mt = current_base_mt.repartition(consolidate_target_partitions, shuffle=True)
        print(f"INFO: current_base_mt repartitioned to {current_base_mt.n_partitions()} partitions. Persisting this state.") # .n_partitions() is an action
        current_base_mt = current_base_mt.persist('MEMORY_AND_DISK') 
        print(f"INFO: Persisted current_base_mt after consolidation. Effective partitions: {current_base_mt.n_partitions()}")

        # --- DIAGNOSTIC PRINTS POST-CONSOLIDATION ---
        print(f"INFO: current_base_mt (post-consolidation and persist):")
        print(f"INFO:   Sample count: {current_base_mt.count_cols()}") # Action
        # print(f"INFO:   Variant count: {current_base_mt.count_rows()}") # Action (optional, can be slow)
        print(f"INFO:   Partitions: {current_base_mt.n_partitions()}") # Action
        
        num_samples_in_mt = current_base_mt.count_cols() # Re-assign for later checks
        if num_samples_in_mt == 0:
            print("FATAL ERROR: current_base_mt has 0 samples after consolidation. This should not happen if filtering was correct.")
            sys.exit(1)
        if hasattr(final_ids_for_mt_df, '__len__') and num_samples_in_mt != len(final_ids_for_mt_df):
             print(f"WARNING: Sample count mismatch post-consolidation! Expected {len(final_ids_for_mt_df)} from ID list, got {num_samples_in_mt} in MT.")
        
        # -------- SIMPLIFIED FINAL REPARTITIONING/ASSIGNMENT --------
        print(f"INFO: Assigning consolidated current_base_mt to final_repartitioned_mt.")
        final_repartitioned_mt = current_base_mt
        mt_current_n_partitions = current_base_mt.n_partitions() 
        print(f"INFO: final_repartitioned_mt is now the consolidated current_base_mt with {mt_current_n_partitions} partitions.")

        # The old 'if/else' block for coalesce is removed as current_base_mt is already repartitioned and persisted.
        
        num_variants_final_mt = final_repartitioned_mt.count_rows() # Action
        print(f"INFO: Final MT prepared. Samples: {num_samples_in_mt}, Variants: {num_variants_final_mt}.")
        print(f"INFO: Final partitions: {final_repartitioned_mt.n_partitions()} (row partitions)")

        print(f"INFO: Writing final prepared Base Cohort MT to: {args.base_cohort_mt_path_out}")
        try:
            final_repartitioned_mt.write(args.base_cohort_mt_path_out, overwrite=True) 
            print("INFO: Base Cohort MT checkpoint successfully written.")

            print("INFO: Verifying written MT checkpoint...")
            mt_check = hl.read_matrix_table(args.base_cohort_mt_path_out)
            mt_check_sample_count = mt_check.count_cols() # Action
            mt_check_variant_count = mt_check.count_rows() # Action
            mt_check_partitions = mt_check.n_partitions()
            print(f"INFO: MT checkpoint verified. Sample count: {mt_check_sample_count}, Variant count: {mt_check_variant_count}, Partitions: {mt_check_partitions}")
            
            if mt_check_sample_count != num_samples_in_mt:
                 print(f"WARNING: Sample count mismatch in final MT checkpoint! Expected {num_samples_in_mt}, got {mt_check_sample_count}")
            if mt_check_variant_count != num_variants_final_mt:
                 print(f"WARNING: Variant count mismatch in final MT checkpoint! Expected {num_variants_final_mt}, got {mt_check_variant_count}")
            if mt_check_partitions != final_repartitioned_mt.n_partitions(): # Check against actual final partitions
                 print(f"WARNING: MT partition count ({mt_check_partitions}) differs from expected ({final_repartitioned_mt.n_partitions()}).")
            del mt_check
        except Exception as e:
            print(f"ERROR: Failed to write or verify final Base Cohort MT checkpoint: {e}")
            sys.exit(1) 
        
        # Unpersist the final MT after it has been written and verified.
        if hasattr(final_repartitioned_mt, '_persisted') and final_repartitioned_mt._persisted:
            print(f"INFO: Unpersisting final_repartitioned_mt (which is current_base_mt).")
            final_repartitioned_mt.unpersist()

        print("--- Base MT Generation Finished ---")
        base_cohort_mt = final_repartitioned_mt 
        # Note: base_cohort_mt is now assigned final_repartitioned_mt. If final_repartitioned_mt was unpersisted, 
        # this means base_cohort_mt is also pointing to an unpersisted MT. This is fine as it's used for 
        # final sample ID extraction if needed, which doesn't require persisted state for cols().select('s').

    if base_cohort_mt is None:
        print(f"FATAL ERROR: Final Base Cohort MT at {args.base_cohort_mt_path_out} has 0 samples.")
        if hail_path_exists(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project):
            delete_gcs_path(args.base_cohort_mt_path_out, project_id_for_billing=args.google_billing_project, recursive=True)
        sys.exit(1)
    print(f"INFO: Base Cohort MT at {args.base_cohort_mt_path_out} is ready with {final_sample_count} samples.\n")

    df_to_save_ids = None
    if 'final_ids_for_mt_df' not in locals() or final_ids_for_mt_df is None or final_ids_for_mt_df.empty:
        print("INFO: 'final_ids_for_mt_df' not directly available for saving. Extracting IDs from final base_cohort_mt (which might be unpersisted).")
        ids_from_mt_ht = base_cohort_mt.cols().select('s') # This should work on an unpersisted MT ref
        df_to_save_ids = ids_from_mt_ht.to_pandas().rename(columns={'s': 'person_id'})
    else:
        print("INFO: Using 'final_ids_for_mt_df' for saving sample IDs.")
        df_to_save_ids = final_ids_for_mt_df.rename(columns={'s': 'person_id'})

    save_sample_ids_to_gcs(df_to_save_ids, args.wgs_ehr_ids_gcs_path_out, fs)

    print(f"INFO: Script completed successfully.")
    print(f"INFO: Output Base MT path: {args.base_cohort_mt_path_out}")
    print(f"INFO: Output Sample ID list path: {args.wgs_ehr_ids_gcs_path_out}")

if __name__ == "__main__":
    main()